{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to EUBUCCO","text":"<p>EUBUCCO is a scientific database of individual building footprints for 322+ million buildings across the 27 European Union countries, Norway, Switzerland, and the UK. It is composed of 55 open datasets, including government registries (62.2%), OpenStreetMap (17.4%), and Microsoft building footprints (20.4%) that have been collected, harmonized, and validated.</p> <p></p>"},{"location":"#attribute-coverage","title":"Attribute Coverage","text":"<p>The database provides high-granularity information for building type, height, floors, and construction year. To maximize utility, EUBUCCO distinguishes between Ground Truth, Merged (from other buildings footprint datasets), and Estimated (inferred with machine learning) attributes.</p> Attribute Ground Truth Merged ML Estimated Total Coverage Type (res/non-res) 38.1% 7.4% 54.5% 100.0% Subtype 17.3% 4.2% 78.5% 100.0% Height 43.2% 0.1% 56.7% 100.0% Floors 16.6% 3.4% 79.9% 100.0% Construction Year 15.6% 0.3% 0.0% 15.9% <ul> <li>Type categories: <code>residential</code> and <code>non-residential</code>.</li> <li>Subtype categories: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, <code>others</code>, <code>detached</code>, <code>semi-detached</code>, <code>terraced</code>, and <code>apartment</code>.</li> </ul>"},{"location":"#accessing-the-data","title":"Accessing the Data","text":"<p>The data is stored as <code>.parquet</code> files on a S3-compatible object storage (MinIO) and can be accessed in multiple ways:</p> Method Best For Description Website Less technical users Download individual region files via a map-based interface. CLI Bulk downloads Sync entire countries or the full dataset to your local storage. Python Quick data exploration Stream specific regions into memory (GeoPandas) or filter chunks (PyArrow). SQL (DuckDB) Advanced Filtering Query the cloud data directly to download only specific subsets (e.g., buildings &gt; 50m). Zenodo Research &amp; Citations Access stable, versioned snapshots for scientific reproducibility."},{"location":"#contribute","title":"Contribute","text":"<p>You know of an open dataset that is not yet included or you've spotted a bug? Drop us an email at info(at)eubucco.com or open an issue in our GitHub repository.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#api-explorer","title":"API Explorer","text":"<p>API Explorer</p> <p>The API Explorer offers an easy way to interact and understand out API. </p>"},{"location":"api/#python-library","title":"Python Library","text":"<p>Soon we will offer our own library to download the data!</p>"},{"location":"license/","title":"License","text":"<p>The EUBUCCO dataset is licensed under the Open Data Commons Open Database License (ODbL) v1.0, with the following regional exceptions:</p> Region <code>geometry_source</code> License Constraint Prague <code>gov-czechia-prague</code> CC-BY-SA Share-Alike Abruzzo <code>gov-italy-abruzzo</code> CC-BY-NC Non-Commercial <p>If you require a dataset licensed exclusively under ODbL, filter out these sources using the <code>geometry_source</code> column.</p>"},{"location":"advanced/conflation/","title":"Conflation Framework","text":"<p>To create a comprehensive and attribute-rich building stock dataset, EUBUCCO utilizes a multi-stage geospatial conflation pipeline. This process integrates fragmented datasets \u2014 authoritative governmental data, volunteered geographic information (OpenStreetMap), and ML-derived footprints (Microsoft) \u2014 into a single, non-redundant database.</p> <p>The framework follows a strict hierarchy prioritizing sources with higher footprint precision and attribute quality.</p> <ol> <li>Governmental Data</li> <li>Volunteered Data (OpenStreetMap)</li> <li>ML-derived Data (Microsoft Building Footprints)</li> </ol> <p> Figure 1: Overview of the EUBUCCO conflation pipeline, illustrating the journey from raw disparate sources to a harmonized, merged building database.</p>"},{"location":"advanced/conflation/#1-data-preprocessing","title":"1. Data Preprocessing","text":"<p>Before matching, data must be harmonized and cleaned to ensure spatial and semantic consistency across all inputs.</p>"},{"location":"advanced/conflation/#spatial-alignment-rubbersheeting","title":"Spatial Alignment (Rubbersheeting)","text":"<p>Datasets often exhibit systematic local shifts. We apply a localized rubbersheeting method to correct positional discrepancies.</p> <ul> <li>Landmark Identification: Reciprocal nearest neighbors are identified within a 25m threshold.</li> <li>Offset Estimation: Neighborhood-level offsets are calculated using H3 grid cells (Resolution 9).</li> <li>Correction: A uniform affine translation is applied to buildings within cells where systematic shifts are detected, correcting offsets &gt;1m in 53% of cases.</li> </ul>"},{"location":"advanced/conflation/#deduplication-and-repair","title":"Deduplication and Repair","text":"<p>Internal consistency is enforced within each source to remove redundant footprints:</p> <ul> <li>Geometry Repair: Invalid or \"broken\" geometries are fixed or removed.</li> <li> <p>Deduplication: We remove buildings that significantly overlap with others in the same dataset. If the IoSA exceeds a tolerance threshold of 0.25, the building with the smaller area is discarded.<sup>1</sup></p> \\[\\text{IoSA}(a, b) = \\frac{|a \\cap b|}{\\min(|a|, |b|)}\\] <p>where \\(|\\cdot|\\) denotes the surface area.</p> </li> </ul>"},{"location":"advanced/conflation/#attribute-cleaning-and-harmonization","title":"Attribute Cleaning and Harmonization","text":"<p>Raw attributes are often noisy or follow local schemas. We implement a rigorous cleaning pipeline:</p> <ul> <li>Outlier Removal: Unrealistic values are filtered out, such as building heights \\(&gt;350m\\), floor counts \\(&gt;100\\), or cases where the floor-to-height ratio suggests a floor height \\(&lt;1.5m\\). Construction years are extracted and clipped to a valid range (i.e. 0\u20132029).</li> <li>Use Type Harmonization: Source-specific tags are mapped to a harmonized use-type classification. This process includes the removal of non-building structures (e.g., traffic areas, radio masts, or underground structures) based on their source classification.</li> </ul>"},{"location":"advanced/conflation/#2-matching","title":"2. Matching","text":"<p>The core of the conflation process is identifying which footprint geometries from different sources refer to the same real-world structure.</p>"},{"location":"advanced/conflation/#candidate-selection","title":"Candidate Selection","text":"<p>To manage computational complexity, we employ a symmetrical k-nearest neighbors (k-NN) search. For every building, we identify the top k=3 potential matches in the secondary dataset within a defined distance threshold and vice-versa.</p>"},{"location":"advanced/conflation/#machine-learning-classifier","title":"Machine Learning Classifier","text":"<p>Unlike traditional methods that use simple \"rule-of-thumb\" intersection thresholds (like IoU &gt; 0.5), EUBUCCO employs a state-of-the-art XGBoost classifier to assess whether two footprint geometries refer to the same real-world structure.</p> <ul> <li>Features: The model is trained on 87 geometric and contextual features, including intersection metrics, shape characteristics (elongation, convexity), and spatial context (neighbor density).</li> <li>Performance: This framework achieves an F1-score of 99.7%, significantly outperforming traditional intersection-based heuristics which often miss 12\u201357% of true matches.</li> </ul> Feature Group Examples Intersection IoU (Intersection over Union), IoA (Intersection over Area), IoSA , Aligned Area Overlap Shape Perimeter, Squareness, Corner Count, Longest Axis, Elongation, Convexity Similarity Centroid Distance, Wall Alignment, Shared Wall Length, Hausdorff Distance Contextual Neighbor counts (5m to 50m), Average alignment in neighborhood, Local density metrics"},{"location":"advanced/conflation/#3-merging","title":"3. Merging","text":"<p>Once matches are identified, the geometries and attributes are integrated to produce the final building record.</p>"},{"location":"advanced/conflation/#attribute-merging","title":"Attribute Merging","text":"<p>Attributes (Type, Height, Floors, Construction Year) are merged across matching blocks using logic designed to maximize spatial consistency. We define the optimal source set \\(\\mathcal{S}_a^*\\) for a target building \\(a\\) as the subset of available buildings \\(\\mathcal{B}_a\\) that maximizes the Intersection over Union (IoU):</p> \\[\\mathcal{S}_a^* = \\arg\\max_{\\mathcal{S} \\subseteq \\mathcal{B}_a} \\text{IoU}(a, \\mathcal{S})\\] <p>where \\(\\mathcal{B}_a\\) represents the set of all matching building footprints from various sources for the target building \\(a\\), and \\(\\mathcal{S}\\) is any possible combination of those footprints.</p>"},{"location":"advanced/conflation/#numerical-attributes","title":"Numerical Attributes","text":"<ul> <li> <p>Merge Aggregation: Building height, floors, and construction year are merged using intersection-area\u2013weighted averages.</p> \\[V_{merged} = \\frac{\\sum_{i \\in \\mathcal{S}_a^*} (V_{source, i} \\cdot w_i)}{\\sum_{i \\in \\mathcal{S}_a^*} w_i}\\] <p>where \\(V_{source, i}\\) is the numerical value from source footprint \\(i\\), and \\(w_i\\) is the area of intersection between the target footprint \\(B_{target}\\) and the \\(i\\)-th matching source footprint \\(B_{source, i}\\).</p> </li> <li> <p>Confidence Scores: To represent the reliability of numerical merges, we provide the range of contributing values:</p> \\[\\text{Range} = [ \\min(V_{source, i}), \\max(V_{source, i}) ] \\quad \\forall i \\in \\mathcal{S}_a^*\\] <p>where \\(\\min\\) and \\(\\max\\) denote the lowest and highest attribute values found among matching source footprints in the optimal source set.</p> </li> </ul>"},{"location":"advanced/conflation/#categorical-attributes","title":"Categorical Attributes","text":"<ul> <li> <p>Merge Aggregation: Building types are assigned based on the dominant category by cumulative intersecting area.</p> \\[C_{merged} = \\arg\\max_{c \\in \\mathcal{C}} \\sum_{i \\in \\mathcal{S}_{a, c}^*} \\text{Area}(B_{target} \\cap B_{source, i})\\] <p>where \\(\\mathcal{C}\\) is the set of possible building categories, and \\(\\mathcal{S}_{a, c}^*\\) is the subset of matching buildings in \\(\\mathcal{S}_a^*\\) that belong to category \\(c\\).</p> </li> <li> <p>Confidence Scores: The confidence score represents the spatial agreement (Intersection over Area) of the source geometries relative to the target:</p> \\[\\text{Confidence} = \\frac{\\text{Area}(B_{target} \\cap \\bigcup_{i \\in \\mathcal{S}_a^* } B_{source, i})}{\\text{Area}(B_{target})}\\] <p>where \\(\\bigcup B_{source, i}\\) represents the geometric union of all matching source footprints, and \\(B_{target}\\) is the geometry of the target footprint.</p> </li> </ul>"},{"location":"advanced/conflation/#geometry-merging","title":"Geometry Merging","text":"<p>Geometries are merged hierarchically based on the source priority. </p> <ul> <li>Block-Level Consistency: We select the best footprint representation for an entire building block from a single source. </li> <li>Avoiding Fragmentation: This prevents the \"stitching together\" of misaligned units from different sources, preserving the structural integrity of the building representation.</li> </ul> <ol> <li> <p>We drop the smaller building since, in many cases, it represents a redundant building part or a sub-structure with special characteristics (e.g., a chimney or tower) that is already accounted for by the larger footprint.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-access/","title":"Accessing the Data","text":"<p>This guide outlines various methods to access and download the EUBUCCO dataset, which is hosted on S3-compatible storage (MinIO).</p>"},{"location":"data-access/#overview-of-data-access-methods","title":"Overview of Data Access Methods","text":"Method Best For Description Website Less technical users Download individual region files via a map-based interface. CLI Bulk downloads Sync entire countries or the full dataset to your local storage. Python Quick data exploration Stream specific regions into memory (GeoPandas) or filter chunks (PyArrow). SQL (DuckDB) Advanced Filtering Query the cloud data directly to download only specific subsets (e.g., buildings &gt; 50m). Zenodo Research &amp; Citations Access stable, versioned snapshots for scientific reproducibility."},{"location":"data-access/cli/","title":"\ud83d\udcbb Command Line Interface (CLI)","text":"<p>EUBUCCO data is hosted on an S3-compatible object store and can be accessed using standard command-line tools. Below are several options depending on your setup and preferences.</p>"},{"location":"data-access/cli/#1-using-aws-cli","title":"1. Using AWS CLI","text":"<p>The standard AWS CLI can be used to access EUBUCCO data by specifying the custom endpoint and using the <code>--no-sign-request</code> flag for anonymous access.</p> Download single regionDownload entire dataset <pre><code># Example: Download Liguria (ITC3)\naws s3 cp s3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet . \\\n    --endpoint-url https://dev-s3.eubucco.com \\\n    --no-sign-request\n</code></pre> <pre><code># Example: Download the entire v0.2 dataset\naws s3 cp s3://eubucco/v0.2/buildings/parquet/ ./eubucco-data/ \\\n    --endpoint-url https://dev-s3.eubucco.com \\\n    --no-sign-request \\\n    --recursive\n</code></pre>"},{"location":"data-access/cli/#2-using-minio-client","title":"2. Using MinIO Client","text":"<p>The MinIO CLI <code>mc</code> is another convenient option for interacting with S3-compatible storage services.</p> <p>Setup: <pre><code>mc alias set eubucco https://dev-s3.eubucco.com \"\" \"\"\n</code></pre></p> Download single regionDownload entire dataset <pre><code>mc cp eubucco/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet .\n</code></pre> <pre><code>mc cp --recursive eubucco/eubucco/v0.2/buildings/parquet/ .\n</code></pre>"},{"location":"data-access/cli/#3-using-standard-network-utilities","title":"3. Using Standard Network Utilities","text":"<p>For users on systems without S3 clients, standard network utilities provide a lightweight alternative for direct file retrieval.</p> wgetcURL <pre><code>wget -c --show-progress https://dev-s3.eubucco.com/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\n</code></pre> <pre><code>curl -OC - --retry 5 https://dev-s3.eubucco.com/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\n</code></pre>"},{"location":"data-access/python/","title":"\ud83d\udc0d Python (GeoPandas / PyArrow)","text":"<p>Python is ideal for quickly streaming data directly into memory for exploration or small-scale analysis.</p>"},{"location":"data-access/python/#1-using-geopandas","title":"1. Using GeoPandas","text":"<p>GeoPandas uses <code>fsspec</code> under the hood to handle S3 paths. Use <code>storage_options</code> to point to the EUBUCCO endpoint.</p> Download single region <pre><code>import geopandas as gpd\n\nstorage_opts = {\n    \"anon\": True,\n    \"client_kwargs\": {\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n}\n\n# Example: Download Liguria (ITC3)\npath = \"s3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\"\ngdf = gpd.read_parquet(path, storage_options=storage_opts)\n</code></pre>"},{"location":"data-access/python/#2-using-pyarrow","title":"2. Using PyArrow","text":"<p>PyArrow is significantly more efficient for large data chunks because it only loads the specific partitions or rows you filter for (predicate pushdown).</p> Download single regionDownload larger chunks <pre><code>import fsspec\nimport pyarrow.dataset as ds\n\nfs = fsspec.filesystem(\n    protocol=\"s3\",\n    anon=True,\n    client_kwargs={\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n)\n\n# Example: Download Liguria (ITC3)\ndataset = ds.dataset(\n    source=\"eubucco/v0.2/buildings/parquet/nuts_id=ITC3\",\n    filesystem=fs,\n    format=\"parquet\",\n)\ntable = dataset.to_table()\n</code></pre> <pre><code>import fsspec\nimport pyarrow.dataset as ds\n\nfs = fsspec.filesystem(\n    protocol=\"s3\",\n    anon=True,\n    client_kwargs={\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n)\n\ndataset = ds.dataset(\n    source=\"eubucco/v0.2/buildings/parquet/\",\n    filesystem=fs,\n    format=\"parquet\"\n)\n\n# Example: Get all data for Italy (regions starting with \"IT\")\ntable = dataset.to_table(filter=ds.field(\"nuts_id\").starts_with(\"IT\"))\n</code></pre> <pre><code># Optionally convert to DataFrame or GeoDataFrame\ndf = table.to_pandas()\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/sql/","title":"\ud83e\udd86 DuckDB (SQL)","text":"<p>DuckDB offers the most powerful way to query EUBUCCO. It treats the S3 bucket like a database, letting you filter by geography before any data is actually downloaded.</p>"},{"location":"data-access/sql/#1-using-sql","title":"1. Using SQL","text":""},{"location":"data-access/sql/#configuration","title":"Configuration","text":"<p>Install spatial extension: <pre><code>INSTALL httpfs; LOAD httpfs;\nINSTALL spatial; LOAD spatial\n</code></pre></p> <p>Configure EUBUCCO's MinIO endpoint: <pre><code>SET s3_endpoint='dev-s3.eubucco.com';\nSET s3_url_style='path';\n</code></pre></p>"},{"location":"data-access/sql/#data-access","title":"Data access","text":"Download single regionDownload larger chunks <pre><code>SELECT * FROM 's3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet' \nLIMIT 10\n</code></pre> <p>By using hive_partitioning, DuckDB understands the folder structure (e.g., <code>nuts_id=...</code>) and can filter hundreds of files instantly.</p> <pre><code>-- Example: Query all of France (FR) for buildings higher than 50 meters\nSELECT id, height, ST_AsText(geometry) \nFROM read_parquet(\n    's3://eubucco/v0.2/buildings/parquet/**/*.parquet',\n    hive_partitioning = true\n)\nWHERE nuts_id LIKE 'FR%' \nAND height &gt; 50;\n</code></pre>"},{"location":"data-access/sql/#2-using-duckdbs-python-library","title":"2. Using DuckDB's Python library","text":"<pre><code>pip install duckdb\n</code></pre> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.execute(\"INSTALL httpfs; LOAD httpfs;\")\ncon.execute(\"INSTALL spatial; LOAD spatial;\")\n\ncon.execute(\"SET s3_endpoint='dev-s3.eubucco.com';\") \ncon.execute(\"SET s3_url_style='path';\")\n</code></pre> <pre><code>query = \"\"\"\nSELECT * EXCLUDE geometry, ST_AsWKB(geometry) AS geometry FROM 's3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet' \nLIMIT 10\n\"\"\"\n# Fetch data with WKB-encoded geometries\ndf = con.execute(query).arrow().to_pandas()\n</code></pre> <pre><code># Optionally convert to GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/website/","title":"\ud83d\uddb1\ufe0f File Downloads via Website","text":"<p>The EUBUCCO Data Portal is the most user-friendly way to browse and download data.</p> <ul> <li>Formats: Available as <code>.parquet</code>, <code>.gpkg</code>, and <code>.shp</code> files</li> <li>Partitioning: The dataset is split into multiple files, each containing all buildings within a single NUTS2 region.</li> <li>Batch download: Larger regions (e.g. federal states or entire countries) can be downloaded as ZIP archives containing all regional NUTS2 files.</li> <li>Metadata: Additional files include information on source datasets, type matching, regional statistics, and more.</li> </ul>"},{"location":"data-access/website/#reading-downloaded-parquet-files","title":"Reading downloaded Parquet files","text":"Read single file (GeoPandas)Read multiple files (GeoPandas)Read multiple files (DuckDB) <pre><code>import geopandas as gpd\n\npath = \"/path/to/downloaded/file.parquet\"\ngdf = gpd.read_parquet(path)\n</code></pre> <pre><code>import geopandas as gpd\n\npath = \"/path/to/downloaded/unzipped/directory\"\ngdf = gpd.read_parquet(path)\n</code></pre> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.execute(\"INSTALL spatial; LOAD spatial;\")\n</code></pre> <pre><code># Read data with WKB-encoded geometries\nquery = \"\"\"\nSELECT\n    * EXCLUDE geometry,\n    ST_AsWKB(geometry) AS geometry\nFROM read_parquet('/path/to/your/folder/*.parquet')\n\"\"\"\ndf = con.execute(query).arrow().to_pandas()\n</code></pre> <pre><code># Optionally convert to GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/zenodo/","title":"\ud83d\udcc2 Scientific Archiving (Zenodo)","text":"<p>For research projects requiring a permanent, citable version of the data, EUBUCCO is archived on Zenodo.</p> <ul> <li>DOI: 10.5281/zenodo.7225259</li> <li>Usage: Ideal for ensuring your research results can be reproduced by others using the exact same data snapshot.</li> <li>Bulk Archive: A full database ZIP (~100 GB) containing all geometries and attributes is available here.</li> </ul>"},{"location":"data-format/","title":"Data Format","text":""},{"location":"data-format/#data-schema","title":"Data Schema","text":"<p>The core dataset structure is organized into five functional groups:</p> <ul> <li>Identifiers: Unique building <code>id</code> and geographic identifies: <code>region_id</code> (NUTS3) and <code>city_id</code> (LAU).</li> <li>Attributes: Building characteristics (<code>type</code>, <code>subtype</code>, <code>height</code>, <code>floors</code>, <code>construction_year</code>).</li> <li>Confidence: Uncertainty of ML-based estimation and cross-source merging of attributes.</li> <li>Sources: Provenance tracking for every attribute (i.e. <code>osm</code>, <code>msft</code>, <code>gov-*</code>, <code>estimated</code>).</li> <li>Geometry: <code>ETRS89</code> footprints stored in Well-Known Binary (WKB) format.</li> </ul>"},{"location":"data-format/#file-format","title":"File Format","text":"<p>The EUBUCCO dataset is distributed in multiple file formats:</p> <ul> <li>Parquet (<code>.parquet</code>)</li> <li>GeoPackage (<code>.gpkg</code>)</li> <li>Shapefile (<code>.shp</code>)</li> </ul>"},{"location":"data-format/#metadata-files","title":"Metadata Files","text":"<p>In addition to the core building data, metadata information with respect to the source datasets, data retrieval dates, and attribute harmonization (e.g. building type mapping and aggregation) are provided as additional files.</p>"},{"location":"data-format/#uncertainty","title":"Uncertainty","text":"<p>For attributes that were estimated using machine learning techniques or merged from OpenStreetMap data, we provide an uncertainty quantification.</p>"},{"location":"data-format/format/","title":"Data File Format","text":"<p>The EUBUCCO dataset is natively optimized for Parquet. While we provide other formats for broad compatibility and convenience, the Parquet distribution is the primary format designed to handle continental-scale analysis with high performance.</p> Parquet GeoPackage Shapefile Support Level Recommended Experimental Experimental Performance Excellent Good Fair Compression High Medium Low Querying Columnar (Best) SQL/Indexed (Good) Flat-file (Poor) Selective Reads Yes (Columns &amp; Rows) Yes (via SQL) No (Full file load)"},{"location":"data-format/format/#parquet-parquet","title":"Parquet (<code>.parquet</code>)","text":"<p>An open-source, columnar storage format optimized for efficient data storage and retrieval.</p> <ul> <li>Best for: Large-scale analysis using Python (Pandas/GeoPandas), R, or Big Data tools.</li> <li>Pros: Smallest file size, fastest to read, and supports selective reads and predicate pushdown filtering.</li> <li>Cons: Limited native support in older GIS desktop software without plugins.</li> </ul> <p>Recommended</p> <p>Native format of the dataset.</p>"},{"location":"data-format/format/#geopackage-gpkg","title":"GeoPackage (<code>.gpkg</code>)","text":"<p>An open, non-proprietary, SQLite-based container for geospatial data.</p> <ul> <li>Best for: General GIS work in QGIS, ArcGIS Pro, or mobile mapping apps.</li> <li>Pros: Single-file convenience, widely compatible, and supports large datasets better than Shapefiles.</li> <li>Cons: Increased file size and slower read speeds compared to Parquet.</li> </ul> <p>Experimental</p> <p>Provided as a convenience for desktop GIS users. Minor typing discrepancies may occur during the automated conversion from the native Parquet source.</p>"},{"location":"data-format/format/#shapefile-shp","title":"Shapefile (<code>.shp</code>)","text":"<p>The legacy industry standard for geospatial vector data.</p> <ul> <li>Best for: Compatibility with older legacy software.</li> <li>Pros: Universal support across almost every GIS tool ever made.</li> <li>Cons: Large files, slow reading, and limited to 2GB per file. No querying possible; all data must be read into memory.</li> </ul> <p>Experimental</p> <p>Provided for legacy GIS software compatibility but generally not recommended. Automated conversion from Parquet may cause minor typing discrepancies. Crucially, some EUBUCCO regions exceed the 2GB limit. Column names differ from Parquet and GeoPackage files due to 10 character limit. Some values have been truncated to 264 characters.</p>"},{"location":"data-format/metadata/","title":"Metadata Files","text":"<p>EUBUCCO includes comprehensive metadata files to support data exploration, validation, analysis, and reproducibility. These files are organized into four main categories:</p> Category Files Description Data Files <code>eubucco_lat_lon.parquet</code> Lightweight building dataset Boundaries <code>LAU-cities-2016.parquet</code>, <code>NUTS-regions-2016.parquet</code> Administrative boundaries used Statistics <code>city-stats.parquet</code>, <code>region-stats.parquet</code>, <code>prediction-eval-metrics.parquet</code> Aggregated statistics and evaluation metrics Reference Tables <code>building-type-harmonization.csv</code>, <code>input-datasets-metadata.xlsx</code> Source dataset information and type mappings"},{"location":"data-format/metadata/#data-files","title":"Data Files","text":""},{"location":"data-format/metadata/#eubucco_lat_lonparquet","title":"<code>eubucco_lat_lon.parquet</code>","text":"<p>A lightweight version of the EUBUCCO v0.2 dataset containing only building centroids (latitude/longitude coordinates) and footprint area, without full footprint geometries.</p>"},{"location":"data-format/metadata/#boundary-files","title":"Boundary Files","text":""},{"location":"data-format/metadata/#lau-cities-2016parquet","title":"<code>LAU-cities-2016.parquet</code>","text":"<p>City (LAU) administrative boundaries used in EUBUCCO v0.2, based on 2016 boundaries.</p> <p>Contents: <code>city_id</code>, <code>region_id</code> and <code>geometry</code> for each city.</p>"},{"location":"data-format/metadata/#nuts-regions-2016parquet","title":"<code>NUTS-regions-2016.parquet</code>","text":"<p>Regional (NUTS 0-3) administrative boundaries used in EUBUCCO v0.2, based on 2016 boundaries.</p> <p>Contents: <code>region_id</code>, <code>region_name</code>, and <code>geometry</code> for each NUTS region at all levels (0-3).</p>"},{"location":"data-format/metadata/#statistics-files","title":"Statistics Files","text":""},{"location":"data-format/metadata/#city-statsparquet","title":"<code>city-stats.parquet</code>","text":"<p>Comprehensive building stock statistics aggregated at the city (LAU) level. This GeoDataFrame includes city geometry and provides detailed metrics for each city.</p>"},{"location":"data-format/metadata/#administrative-information","title":"Administrative Information","text":"<ul> <li><code>city_id</code>, <code>region_id</code> (NUTS 3), <code>country</code></li> <li>City geometry (CRS: EPSG:3035)</li> </ul>"},{"location":"data-format/metadata/#building-counts-by-source","title":"Building Counts by Source","text":"<ul> <li><code>n_gov</code>, <code>n_osm</code>, <code>n_msft</code> \u2014 Total counts by geometry source</li> </ul>"},{"location":"data-format/metadata/#data-quality-indicators","title":"Data Quality Indicators","text":"<ul> <li>Ground truth counts: Buildings with attributes from the same source as geometry</li> <li><code>n_gt_type</code>, <code>n_gt_subtype</code>, <code>n_gt_height</code>, <code>n_gt_floors</code>, <code>n_gt_construction_year</code></li> <li>Merged attribute counts: Buildings with attributes merged from different sources</li> <li><code>n_merged_type</code>, <code>n_merged_subtype</code>, <code>n_merged_height</code>, <code>n_merged_floors</code>, <code>n_merged_construction_year</code></li> <li>Estimated attribute counts: Buildings with estimated attributes</li> <li><code>n_estimated_type</code>, <code>n_estimated_subtype</code>, <code>n_estimated_height</code>, <code>n_estimated_floors</code></li> </ul>"},{"location":"data-format/metadata/#building-type-distributions","title":"Building Type Distributions","text":"<ul> <li>Main types: Residential, non-residential (counts and areas)</li> <li>Subtypes: Commercial, industrial, agricultural, public, others, detached, semi-detached, terraced, apartment (counts and areas)</li> </ul>"},{"location":"data-format/metadata/#attribute-distributions","title":"Attribute Distributions","text":"<ul> <li>Height bins: 0-5m, 5-10m, 10-20m, &gt;20m</li> <li>Floor bins: 0-3, 4-6, &gt;6 floors</li> <li>Construction year bins: \u22641900, 1901-1970, 1971-2000, &gt;2000</li> <li>Footprint area bins: 0-25m\u00b2, 25-100m\u00b2, 100-500m\u00b2, &gt;500m\u00b2</li> </ul>"},{"location":"data-format/metadata/#area-metrics","title":"Area Metrics","text":"<ul> <li>Total footprint area and floor area</li> <li>Breakdowns by source (gov, osm, msft), type, and subtype</li> </ul>"},{"location":"data-format/metadata/#region-statsparquet","title":"<code>region-stats.parquet</code>","text":"<p>Building stock statistics aggregated at the regional (NUTS 3) level. Contains the same metrics as city-level statistics but aggregated to NUTS 3 regions, including regional geometry.</p>"},{"location":"data-format/metadata/#prediction-eval-metricsparquet","title":"<code>prediction-eval-metrics.parquet</code>","text":"<p>Regional (NUTS 2) evaluation metrics for building attribute estimation models. This GeoDataFrame provides comprehensive performance metrics for predicted building attributes.</p>"},{"location":"data-format/metadata/#administrative-information_1","title":"Administrative Information","text":"<ul> <li><code>region_id</code> (NUTS 2), <code>country</code></li> <li>Regional geometry (CRS: EPSG:3035)</li> </ul>"},{"location":"data-format/metadata/#sample-sizes","title":"Sample Sizes","text":"<ul> <li><code>n</code> \u2014 Total number of buildings</li> <li><code>n_gt_binary_type</code>, <code>n_gt_type</code>, <code>n_gt_residential_type</code>, <code>n_gt_height</code>, <code>n_gt_floors</code> \u2014 Ground truth counts per attribute</li> </ul>"},{"location":"data-format/metadata/#categorical-variable-metrics","title":"Categorical Variable Metrics","text":"<p>For binary_type, type, and residential_type:</p> <ul> <li>Overall classification metrics: F1 score (macro and micro), Cohen's kappa</li> <li>Per-class F1 scores: Individual F1 scores for each building type/subtype</li> </ul>"},{"location":"data-format/metadata/#continuous-variable-metrics","title":"Continuous Variable Metrics","text":"<p>For height and floors:</p> <ul> <li>Overall metrics: MAE, RMSE, R\u00b2 score</li> <li>Binned metrics: MAE and RMSE by value ranges</li> <li>Height: 0-5m, 5-10m, 10-20m, &gt;20m</li> <li>Floors: 0-3, 3-6, &gt;6</li> </ul>"},{"location":"data-format/metadata/#external-validation","title":"External Validation","text":"<p>For height:</p> <ul> <li>Microsoft height comparison: Metrics comparing predicted height with heights from Microsoft's GlobalMLBuildingFootprints dataset</li> </ul>"},{"location":"data-format/metadata/#reference-tables","title":"Reference Tables","text":""},{"location":"data-format/metadata/#building-type-harmonizationcsv","title":"<code>building-type-harmonization.csv</code>","text":"<p>Mapping table showing how building types from various source datasets are harmonized to the standardized EUBUCCO building type classification.</p>"},{"location":"data-format/metadata/#input-datasets-metadataxlsx","title":"<code>input-datasets-metadata.xlsx</code>","text":"<p>Comprehensive metadata table providing detailed information about all source datasets used in EUBUCCO v0.2.</p>"},{"location":"data-format/metadata/#contents","title":"Contents","text":"<ul> <li>Dataset identification: Name, country, geographic coverage</li> <li>Access information: Data owner, license, access date, download links or procedures</li> <li>Technical details: File format, data structure, attribute availability</li> <li>Integration information: Processing workflow and integration approach</li> </ul>"},{"location":"data-format/partitioning/","title":"Data Partitioning and Sorting","text":"<p>EUBUCCO Parquet files use a specific physical layout to optimize data access and minimize I/O overhead for large-scale geospatial analysis.</p>"},{"location":"data-format/partitioning/#partitioning","title":"Partitioning","text":"<p>The dataset uses Hive-style partitioning by NUTS2 regions, where each region is stored in its own directory.</p> <ul> <li>Format: <code>eubucco/v0.2/buildings/parquet/nuts_id=&lt;ID&gt;/&lt;ID&gt;.parquet</code></li> </ul> <p>Benefits:</p> <ul> <li>Partition Pruning: Query engines can skip entire directories when filtering by region, dramatically reducing I/O</li> <li>Selective Access: Download or process only specific geographic regions</li> </ul> <p>For examples of leveraging partitioning in queries, see Efficient Data Loading &amp; Filtering.</p> <p>Partition Key vs. Data Column</p> <p>The partition key <code>nuts_id</code> (NUTS2) corresponds to the parent region of the <code>region_id</code> (NUTS3) column in the dataset.</p>"},{"location":"data-format/partitioning/#sorting","title":"Sorting","text":"<p>Within each partition file, rows are sorted using a multi-level hierarchy to optimize data locality:</p> <ol> <li>Source Type: Grouped by dataset origin (<code>gov</code>, <code>msft</code>, <code>osm</code>)</li> <li>NUTS3 Region: Sub-regional grouping via <code>region_id</code></li> <li>City ID (LAU): Local administrative unit via <code>city_id</code></li> </ol> <p>Benefits:</p> <ul> <li>Spatial Locality: Buildings in the same city are stored contiguously, improving cache efficiency</li> <li>Predicate Pushdown: Row group statistics enable efficient spatial filtering without reading entire files</li> <li>Source Grouping: Buildings from the same source are clustered together, facilitating source-based filtering</li> </ul> <p>How Sorting Enables Efficient Filtering</p> <p>By sorting down to the City ID level, the dataset achieves a high degree of spatial clustering. Parquet files are internally divided into row groups, each containing metadata with min/max statistics for columns. The spatial sorting ensures that buildings within the same geographic area (city) are stored in the same row groups, allowing query engines to examine row group metadata and skip entire row groups that fall outside query bounds without reading their data.</p> <p>In practice, this provides bounding box (BBOX) filtering performance nearly equivalent to more complex spatial indices (like Hilbert or Z-order curves) while maintaining a simpler, human-readable hierarchy.</p>"},{"location":"data-format/schema/","title":"Data Schema Reference","text":""},{"location":"data-format/schema/#core-data-fields","title":"Core Data Fields","text":"Group Attribute Type Definition Example Identifiers <code>id</code> <code>string</code> Unique ID composed of a Block ID<sup>1</sup> and sequence number. <code>3ba70f9963714924-0</code> <code>region_id</code> <code>string</code> NUTS3 regional identifier. <code>FR931</code> <code>city_id</code> <code>string</code> Local Administrative Unit (LAU) identifier for the city. <code>FR93066</code> Attributes <code>type</code> <code>category</code> Binary usage type<sup>2</sup>. <code>residential</code> <code>subtype</code> <code>category</code> Detailed usage type<sup>3</sup>. <code>terraced</code> <code>height</code> <code>float</code> Distance in meters from ground floor to top of building. <code>23.9</code> <code>floors</code> <code>float</code> Total number of above-ground floors. <code>3.5</code> <code>construction_year</code> <code>integer</code> Year initial construction finished (not renovation year). <code>1963</code> Geometry <code>geometry</code> <code>binary</code> Footprint geometry projected in <code>ETRS89</code> (<code>EPSG:3035</code>) encoded as WKB. Units in meters. <code>01030000...</code>"},{"location":"data-format/schema/#auxiliary-data-fields","title":"Auxiliary Data Fields","text":"Group Attribute Type Definition Example Confidence <code>type_confidence</code> <code>float</code> Area-weighted intersection ratio (merged) or sum of calibrated subtype probabilities (predicted). Range: [0, 1]. <code>0.95</code> <code>subtype_confidence</code> <code>float</code> Area-weighted intersection ratio (merged) or calibrated class probability (predicted). Range: [0, 1]. <code>0.64</code> <code>height_confidence_lower</code> <code>float</code> Min source value (merged) or lower 95% bootstrap CI (predicted) for height. <code>7.5</code> <code>height_confidence_upper</code> <code>float</code> Max source value (merged) or upper 95% bootstrap CI (predicted) for height. <code>10.5</code> <code>floors_confidence_lower</code> <code>float</code> Min source value (merged) or lower 95% bootstrap CI (predicted) for floors. <code>2.7</code> <code>floors_confidence_upper</code> <code>float</code> Max source value (merged) or upper 95% bootstrap CI (predicted) for floors. <code>3.2</code> <code>construction_year_confidence_lower</code> <code>int</code> Min source year (merged) or lower 95% bootstrap CI (predicted) for year. <code>1990</code> <code>construction_year_confidence_upper</code> <code>int</code> Max source year (merged) or upper 95% bootstrap CI (predicted) for year. <code>2000</code> Sources <code>geometry_source</code> <code>category</code> Origin of footprint geometry. <code>gov-france</code> <code>type_source</code> <code>category</code> Origin of <code>type</code> attribute. <code>osm</code> <code>subtype_source</code> <code>category</code> Origin of <code>subtype</code> attribute. <code>estimated</code> <code>height_source</code> <code>category</code> Origin of <code>height</code> attribute. <code>estimated</code> <code>floors_source</code> <code>category</code> Origin of <code>floors</code> attribute. <code>gov-france</code> <code>construction_year_source</code> <code>category</code> Origin of <code>construction_year</code> attribute. <code>gov-france</code> Source IDs <code>geometry_source_id</code> <code>string</code> Primary identifier from the source provider. <code>BATIMENT000...</code> <code>type_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>type</code> attribute. <code>['osm_123']</code> <code>subtype_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>subtype</code> attribute. <code>['osm_123']</code> <code>height_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>height</code> attribute. <code>['ign_456']</code> <code>floors_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>floors</code> attribute. <code>['ign_456']</code> <code>construction_year_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>construction_year</code>. <code>['ign_456']</code> Source values <code>subtype_raw</code> <code>string</code> The original, unmapped building use type from the source dataset. <code>Einfamilienhaus</code> <p>Incorrect OSM Source IDs</p> <p>The <code>geometry_source_id</code> and <code>&lt;attr&gt;_source_ids</code> fields currently do not contain the original OSM ID but a sequential index.</p>"},{"location":"data-format/schema/#example","title":"Example","text":"id region_id city_id type subtype height floors construction_year type_confidence subtype_confidence height_confidence_lower height_confidence_upper floors_confidence_lower floors_confidence_upper construction_year_confidence_lower construction_year_confidence_upper geometry_source type_source subtype_source height_source floors_source construction_year_source geometry_source_id type_source_ids subtype_source_ids height_source_ids floors_source_ids construction_year_source_ids subtype_raw geometry aec95c88db604a13-0 FRF31 FR54029 residential detached 3.3 1.0 nan 1.0 0.99 3.2 3.5 1.0 1.0 &lt;NA&gt; &lt;NA&gt; gov-france osm estimated estimated osm &lt;NA&gt; BATIMENT0000000334652469 ['lorraine-latest_1372503'] &lt;NA&gt; &lt;NA&gt; ['lorraine-latest_1372503'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((402...)) 9d1c73d618ef419e-0 FRF31 FR54037 residential detached 5.7 2.0 nan 0.86 0.84 5.3 6.2 2.0 2.0 &lt;NA&gt; &lt;NA&gt; gov-france osm estimated estimated osm &lt;NA&gt; BATIMENT0000002101827291 ['lorraine-latest_1661552'] &lt;NA&gt; &lt;NA&gt; ['lorraine-latest_1661552'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((404...)) ce19c9e4976d46f6-2 FRF31 FR54039 non-residential others 3.8 2.0 nan 0.84 0.84 nan nan 2.0 2.0 &lt;NA&gt; &lt;NA&gt; gov-france osm osm gov-france osm &lt;NA&gt; BATIMENT0000000334989890 ['lorraine-latest_805209'] ['lorraine-latest_805209'] &lt;NA&gt; ['lorraine-latest_804619'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((407...)) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ..."},{"location":"data-format/schema/#technical-notes","title":"Technical Notes","text":""},{"location":"data-format/schema/#building-type-classification","title":"Building Type Classification","text":"<ul> <li>We use a two-tier hierarchy to categorize building usage. The <code>type</code> field provides a high-level binary split, while the <code>subtype</code> field contains the granular class.<ul> <li>Type: <code>residential</code>. Subtypes: <code>detached</code> (single-family), <code>semi-detached</code> (duplex), <code>terraced</code> (row house), and <code>apartment</code> (multi-family).</li> <li>Type: <code>non-residential</code>. Subtypes: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, and <code>others</code> (e.g. garages)</li> </ul> </li> <li>The <code>subtype_raw</code> column preserves the original source classification string prior to this harmonization. Refer to the metadata file on building type mapping for more details.</li> </ul>"},{"location":"data-format/schema/#geometry-encoding","title":"Geometry Encoding","text":"<ul> <li>The <code>geometry</code> column is stored as Well-Known Binary (WKB). This is a compact, machine-readable format optimized for GIS tools like PostGIS, QGIS, and GeoPandas.</li> </ul>"},{"location":"data-format/schema/#attribute-sources","title":"Attribute Sources","text":"<ul> <li>Attribute source categories are <code>osm</code>, <code>msft</code>, <code>gov-&lt;region&gt;</code>, <code>estimated</code> (where  represents the specific regional authoritative dataset identifier)"},{"location":"data-format/schema/#attribute-uncertainty","title":"Attribute Uncertainty","text":"<ul> <li>Ground Truth: <code>NaN</code> in a <code>&lt;attr&gt;_confidence</code> column indicates the attribute was provided directly by the geometry source; no merging or machine learning estimation was required.</li> <li>Interpretation: For <code>type_confidence</code> and <code>subtype_confidence</code>, a value of 0.6 implies that 60% of buildings in that cohort are statistically expected to be correctly classified.</li> <li>Methodology: For details on how the uncertainty of attribute estimation and merging is quantified, please refer to the Uncertainty Section.</li> </ul>"},{"location":"data-format/schema/#attribute-merging","title":"Attribute Merging","text":"<ul> <li>Source Mismatch: If <code>geometry_source</code> and <code>&lt;attr&gt;_source</code> differ, the attribute has been merged between datasets.</li> <li>Data Fusion: If <code>&lt;attr&gt;_source_ids</code> contains multiple values, the final value has been from aggregated from multiple source buildings.</li> </ul> <ol> <li> <p>Block ID: A unique identifier for a cluster of topologically connected (touching) buildings.\u00a0\u21a9</p> </li> <li> <p>Binary use type categories: <code>residential</code> and <code>non-residential</code>.\u00a0\u21a9</p> </li> <li> <p>Detailed use type categories: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, <code>others</code>, <code>detached</code>, <code>semi-detached</code>, <code>terraced</code>, and <code>apartment</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-format/uncertainty/","title":"Attribute Uncertainty Quantification","text":"<p>Building attributes derived from source data (e.g., OpenStreetMap) or machine learning models inherently carry degrees of uncertainty.</p> <p>To provide a transparent measure of data reliability, each attribute is accompanied by a confidence metric. This documentation details the dual-methodology approach\u2014spatial intersection ratios for merged records and classification probability / regression bootstrapping for predicted values\u2014used to quantify this uncertainty.</p>"},{"location":"data-format/uncertainty/#type-subtype","title":"Type &amp; Subtype","text":"<p>The confidence of the categorical attributes type and subtype is quantified as follows:</p>"},{"location":"data-format/uncertainty/#for-attributes-merged-from-osm-footprint-intersection-ratio","title":"For Attributes Merged From OSM: Footprint Intersection Ratio","text":"<p>When an attribute is merged from a source dataset (i.e. OSM) onto a target building footprint, the confidence is calculated as the Intersection over Area (IoA):</p> \\[\\text{Confidence} = \\frac{\\text{Area}(B_{target} \\cap B_{source})}{\\text{Area}(B_{target})}\\] <ul> <li>1.0: Perfect spatial overlap.</li> <li>&lt; 1.0: Partial overlap, suggesting potential mismatch or misaligned geometries.</li> </ul>"},{"location":"data-format/uncertainty/#for-attributes-predicted-using-ml-calibrated-classification-probabilities","title":"For Attributes Predicted using ML: Calibrated Classification Probabilities","text":"<p>For attributes generated by our classification models, the confidence is the calibrated probability of the predicted class:</p> <ul> <li>Subtype Confidence: The output of the model after applying calibration to ensure the probability reflects real-world accuracy.</li> <li> <p>Type Confidence: Since type (<code>residential</code>/<code>non-residential</code>) is an aggregate, its confidence is the sum of the calibrated probabilities of all subtypes belonging to that category:</p> \\[P(\\text{Type}) = \\sum P(\\text{Subtypes} \\in \\text{Type})\\] </li> </ul>"},{"location":"data-format/uncertainty/#height-floors-construction-year","title":"Height, Floors, Construction Year","text":"<p>The confidence of the numerical attributes height, floors and construction year is quantified as follows:</p>"},{"location":"data-format/uncertainty/#for-attributes-merged-from-osm-value-extremes","title":"For Attributes Merged From OSM: Value Extremes","text":"<p>If multiple source buildings match a single target footprint with different values:</p> <ul> <li>Lower: The minimum value found among all matching sources.</li> <li>Upper: The maximum value found among all matching sources.</li> </ul>"},{"location":"data-format/uncertainty/#for-attributes-predicted-using-ml-bootstrapped-95-ci","title":"For Attributes Predicted using ML: Bootstrapped 95% CI","text":"<p>To quantify the uncertainty of our regression models, we use a bootstrap approach:</p> <ol> <li>Bootstrap Sampling: The model is run \\(n=10\\) times using different seeds or data subsamples, resulting in a set of predictions \\(Y = \\{y_1, y_2, \\dots, y_{10}\\}\\).</li> <li> <p>Standard Error Calculation: We calculate the sample mean \\(\\bar{y}\\) and the standard error of the mean (SEM):</p> \\[\\text{SEM} = \\frac{s}{\\sqrt{n}}\\] <p>where \\(s\\) is the sample standard deviation.</p> </li> <li> <p>Interval Calculation: The confidence bounds are defined using the \\(t\\)-statistic for \\(n-1\\) degrees of freedom (at \\(\\alpha = 0.05\\)):</p> <p>Lower Confidence:</p> \\[\\text{lower} = \\bar{y} - t_{0.975, 9} \\times \\text{SEM}\\] <p>Upper Confidence:</p> \\[\\text{upper} = \\bar{y} + t_{0.975, 9} \\times \\text{SEM}\\] </li> </ol>"},{"location":"data-usage/cookbook/","title":"Cookbook: Python &amp; SQL","text":"<p>This reference provides essential snippets for cleaning, enriching, and transforming EUBUCCO data using GeoPandas or DuckDB.</p>"},{"location":"data-usage/cookbook/#identifiers","title":"Identifiers","text":""},{"location":"data-usage/cookbook/#determining-block-id","title":"Determining Block ID","text":"<p>EUBUCCO IDs are formatted as <code>{uuid}-{index}</code>. The prefix can be extracted to identify building blocks, i.e. clusters of adjacent buildings.</p> GeoPandasDuckDB <pre><code>gdf['block_id'] = gdf['id'].str.split('-').str[0]\n</code></pre> <pre><code>SELECT SPLIT_PART(id, '-', 1) AS block_id FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#determining-nuts-0-1-or-2-region","title":"Determining NUTS 0, 1, or 2 Region","text":"GeoPandasDuckDB <pre><code>gdf['NUTS_2'] = gdf['region_id'].str[:4]\ngdf['NUTS_1'] = gdf['region_id'].str[:3]\ngdf['country'] = gdf['region_id'].str[:2]  # EU VAT 2-digit country code\n</code></pre> <pre><code>SELECT LEFT(region_id, 2) AS country FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#attribute-metadata","title":"Attribute Metadata","text":""},{"location":"data-usage/cookbook/#handling-confidence-values","title":"Handling Confidence Values","text":"<p>Authoritative data lacks explicit confidence scores. Fill these gaps with 1.0 to ensure they are not excluded during quality filters.</p> GeoPandasDuckDB <pre><code>gdf[\"type_confidence\"] = gdf[\"type_confidence\"].fillna(1.0)\n</code></pre> <pre><code>SELECT COALESCE(type_confidence, 1.0) as type_confidence FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#attribute-source-comparison","title":"Attribute Source Comparison","text":"<p>Determine if an attribute was merged from an external source or estimated using ML by identifying mismatches between geometry and attribute sources.</p> GeoPandasDuckDB <pre><code>gdf[\"is_inferred\"] = gdf[\"geometry_source\"] != gdf[\"type_source\"]\n</code></pre> <pre><code>SELECT *, (geometry_source != type_source) AS is_inferred FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#custom-building-type-harmonization","title":"Custom Building Type Harmonization","text":"<p>Map raw subtypes from source datasets to custom building type classification.</p> GeoPandasDuckDB <pre><code>osm_map = {\n    'apartments': 'high_density',\n    'detached': 'low_density',\n    'retail': 'economic'\n}\ngdf['urban_function'] = gdf['subtype_raw'].map(osm_map).fillna('unclassified')\n</code></pre> <pre><code>SELECT \n    subtype_raw,\n    CASE \n        WHEN subtype_raw IN ('apartments', 'terrace') THEN 'high_density'\n        WHEN subtype_raw IN ('detached', 'house') THEN 'low_density'\n        WHEN subtype_raw IN ('retail', 'office') THEN 'economic'\n        ELSE 'unclassified' \n    END AS urban_function\nFROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#geometry","title":"Geometry","text":""},{"location":"data-usage/cookbook/#decoding-wkb-and-wkt","title":"Decoding WKB and WKT","text":"<p>Geometries are stored as Well-Known Binary (WKB). Use these methods for manual decoding or to export human-readable Well-Known Text (WKT).</p> Pandas (WKB Parsing)DuckDB (WKB Export)DuckDB (WKT Export) <pre><code>import pandas as pd\nimport geopandas as gpd\nfrom shapely import wkb\n\n# Load raw parquet (geometry is binary)\ndf = pd.read_parquet(\"data.parquet\")\n\n# Fast decoding of WKB column to Shapely objects\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]),\n    # OR geometry=df[\"geometry\"].apply(wkb.loads),\n    crs=\"EPSG:3035\"\n)\n</code></pre> <pre><code>SELECT id, ST_AsWKB(geometry) AS geometry FROM buildings;\n</code></pre> <pre><code>-- Convert binary to human-readable strings\nSELECT id, ST_AsText(geometry) AS geometry FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#coordinate-reference-system-crs-transformation","title":"Coordinate Reference System (CRS) Transformation","text":"<p>Convert building geometries from the local projected CRS (<code>EPSG:3035</code>) to WGS84 (Lat/Lng).</p> GeoPandasDuckDB <pre><code>gdf = gdf.to_crs(epsg=4326)\n</code></pre> <pre><code>SELECT \n    * EXCLUDE geometry, \n    ST_Transform(geometry, 'EPSG:3035', 'EPSG:4326') AS geometry \nFROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#centroid-generation","title":"Centroid Generation","text":"<p>Replace building footprints with centroids to reduce computational overhead for point-in-polygon operations or visualization.</p> GeoPandasDuckDB <pre><code>gdf[\"geometry\"] = gdf.centroid\n</code></pre> <pre><code>SELECT * EXCLUDE geometry, ST_Centroid(geometry) AS geometry FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#h3-grid-aggregation","title":"H3 Grid Aggregation","text":"<p>Map buildings to hexagonal grid (H3) for analysis.</p> Python (h3pandas)Python (h3-py + GeoPandas)DuckDB <pre><code>import h3pandas\n# Automatically handles centroid extraction and CRS shift to WGS84\n# 'res=9' provides a spatial resolution of ~0.1km\u00b2\nh3_gdf = gdf.h3.geo_to_h3_aggregate(res=9, operation='count')\n</code></pre> <pre><code>import h3\n# Manual approach: Transform -&gt; Centroid -&gt; Map\ngdf['h3_9'] = gdf.centroid.to_crs(epsg=4326).apply(\n    lambda g: h3.latlng_to_cell(g.y, g.x, 9)\n)\nh3_stats = gdf.groupby('h3_9').size()\n</code></pre> <pre><code>-- Transform to EPSG:4326 within the H3 function call\nSELECT \n    h3_latlng_to_cell(\n        ST_Y(ST_Transform(ST_Centroid(geometry), 'EPSG:3035', 'EPSG:4326')), \n        ST_X(ST_Transform(ST_Centroid(geometry), 'EPSG:3035', 'EPSG:4326')), \n        9\n    ) AS cell,\n    COUNT(*) AS count\nFROM buildings\nGROUP BY cell;\n</code></pre>"},{"location":"data-usage/loading/","title":"Efficient Data Loading &amp; Filtering","text":"<p>Filtering at the data-loading stage is the most efficient way to extract specific building subsets based on quality, source, or geography. By using predicate pushdown across partitioned files, you avoid reading unnecessary data into memory.</p> <p>Prerequisites</p> <p>The following commands assume that you have downloaded the complete EUBUCCO dataset, e.g. using the CLI: <pre><code>aws s3 cp s3://eubucco/v0.2/buildings/parquet/ ./eubucco-data/ \\\n    --endpoint-url https://dev-s3.eubucco.com \\\n    --no-sign-request \\\n    --recursive\n</code></pre></p>"},{"location":"data-usage/loading/#spatial-filtering","title":"Spatial Filtering","text":""},{"location":"data-usage/loading/#bounding-box-filtering","title":"Bounding Box Filtering","text":"<p>Limit the dataset to a specific geographic extent using helper columns. </p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\nimport glob\n\n# GeoPandas only reads the subsets of the data that intersect the bbox\nbbox = (5300000, 1880000, 5400000, 1920000)\ngdf = gpd.read_parquet(\"eubucco_data/\", bbox=bbox)\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\n\ntable = dataset.to_table(filter=(\n    (ds.field(\"bbox.xmin\") &gt;= 5300000) &amp; (ds.field(\"bbox.xmax\") &lt;= 5400000) &amp;\n    (ds.field(\"bbox.ymin\") &gt;= 1880000) &amp; (ds.field(\"bbox.ymax\") &lt;= 1920000)\n))\n</code></pre> <pre><code>-- Use the glob pattern '**/*.parquet' to scan all regional files\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE bbox.xmin &gt;= 5300000 AND bbox.xmax &lt;= 5400000\n  AND bbox.ymin &gt;= 1880000 AND bbox.ymax &lt;= 1920000;\n</code></pre>"},{"location":"data-usage/loading/#administrative-regions-filtering","title":"Administrative Regions Filtering","text":"<p>Limit the dataset to specific cities, regions, or countries using the partition keys and <code>region_id</code> (NUTS 0/1/2) or <code>city_id</code> row group metadata.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\n# Filter for a specific city\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"city_id\", \"==\", \"EL22040104\")])\n\n# Filter for all regions e.g. in Greece [EL]\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"region_id\", \"&gt;=\", \"EL\"), (\"region_id\", \"&lt;\", \"EM\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\n\n# Filter for a specific city\ncity_table = dataset.to_table(filter=ds.field(\"city_id\") == \"EL22040104\")\n\n# Filter for all regions (e.g. in Greece [EL])\ncountry_table = dataset.to_table(filter=ds.field(\"region_id\").starts_with(\"EL\"))\n</code></pre> <pre><code>-- Querying a specific city\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE city_id = 'EL22040104';\n\n-- Querying all regions e.g. in Greek [EL]\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE region_id LIKE 'EL%';\n</code></pre>"},{"location":"data-usage/loading/#source-filtering","title":"Source Filtering","text":""},{"location":"data-usage/loading/#filtering-by-footprint-source","title":"Filtering by Footprint Source","text":"<p>Discard ML-derived footprints (i.e. Microsoft Footprints).</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"geometry_source\", \"!=\", \"msft\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=ds.field(\"geometry_source\") != \"msft\")\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source != 'msft';\n</code></pre> <p>Filter for governmental data / discard non-authoritative sources (i.e. OSM and Microsoft ML).</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"geometry_source\", \"not in\", [\"msft\", \"osm\"])])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=~ds.field(\"geometry_source\").isin([\"msft\", \"osm\"]))\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source NOT IN ('msft', 'osm');\n</code></pre>"},{"location":"data-usage/loading/#filtering-by-attribute-source","title":"Filtering by Attribute Source","text":"<p>Isolate records where the specific attribute was not estimated using ML.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"type_source\", \"!=\", \"estimated\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=ds.field(\"type_source\") != \"estimated\")\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE type_source != 'estimated';\n</code></pre> <p>Isolate records where the specific attribute comes from the same source as the footprint geometry (i.e. the attribute was neither merged nor estimated using ML).</p> DuckDB <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source = type_source;\n</code></pre>"},{"location":"data-usage/loading/#column-filtering","title":"Column Filtering","text":"<p>Select only the footprint geometry and the main building attributes, and discard the remaining metadata columns.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ncolumns = [\n    \"id\", \"region_id\", \"city_id\", \n    \"type\", \"subtype\", \"height\", \"floors\", \n    \"construction_year\", \"geometry\"\n]\n\ngdf = gpd.read_parquet(\"eubucco_data/\", columns=columns)\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ncolumns = [\n    \"id\", \"region_id\", \"city_id\", \n    \"type\", \"subtype\", \"height\", \"floors\",\n    \"construction_year\", \"geometry\"\n]\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(columns=columns)\n</code></pre> <pre><code>SELECT \n    id,\n    region_id,\n    city_id,\n    type,\n    subtype,\n    height,\n    floors,\n    construction_year,\n    ST_AsText(geometry) AS geometry\nFROM read_parquet('eubucco_data/**/*.parquet');\n</code></pre>"},{"location":"data-usage/loading/#confidence-filtering","title":"Confidence Filtering","text":"<p>Discard buildings with merged or estimated attributes that carry high uncertainty. We treat authoritative data (where confidence is <code>NaN</code>) as 100% certain.</p> <p>Performance: Since data isn't partitioned or sorted by confidence, metadata-based skipping is unavailable. However, filtering while reading in small buffers keeps memory usage low.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\")\n\n# Categorical: Type confidence &gt; 80%\nhigh_conf = gdf[gdf[\"type_confidence\"].fillna(1.0) &gt; 0.8]\n\n# Numerical: Precise height (uncertainty interval &lt; 2m)\nprecise_height = gdf[(gdf[\"height_confidence_upper\"] - gdf[\"height_confidence_lower\"]) &lt; 2.0]    \n</code></pre> <pre><code>import pyarrow.dataset as ds\n\n# Categorical: Type confidence &gt; 80%\nprecise_type = (ds.field(\"type_confidence\") &gt; 0.8) | ds.field(\"type_confidence\").is_null()\n\n# Numerical: Precise height (uncertainty interval &lt; 2m)\nheight_spread = ds.field(\"height_confidence_upper\") - ds.field(\"height_confidence_lower\")\nprecise_height = (height_spread &lt; 2.0) | ds.field(\"height_confidence_upper\").is_null()\n\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=(precise_type &amp; precise_height))\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \n-- Categorical confidence\nWHERE COALESCE(type_confidence, 1.0) &gt; 0.8 \n-- Numerical confidence\nAND (height_confidence_upper - height_confidence_lower) &lt; 2.0;\n</code></pre>"}]}