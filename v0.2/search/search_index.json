{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to EUBUCCO","text":"<p>EUBUCCO is a scientific database of individual building footprints for 322+ million buildings across the 27 European Union countries, Norway, Switzerland, and the UK. It is composed of 55 open datasets, including government registries (62.2%), OpenStreetMap (17.4%), and Microsoft building footprints (20.4%) that have been collected, harmonized, and validated.</p> <p></p>"},{"location":"#attribute-coverage","title":"Attribute Coverage","text":"<p>The database provides high-granularity information for building type, height, floors, and construction year. To maximize utility, EUBUCCO distinguishes between Ground Truth, Merged (from other buildings footprint datasets), and Estimated (inferred with machine learning) attributes.</p> Attribute Ground Truth Merged ML Estimated Total Coverage Type (res/non-res) 38.1% 7.4% 54.5% 100.0% Subtype 17.3% 4.2% 78.5% 100.0% Height 43.2% 0.1% 56.7% 100.0% Floors 16.6% 3.4% 79.9% 100.0% Construction Year 15.6% 0.3% 0.0% 15.9% <ul> <li>Type categories: <code>residential</code> and <code>non-residential</code>.</li> <li>Subtype categories: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, <code>others</code>, <code>detached</code>, <code>semi-detached</code>, <code>terraced</code>, and <code>apartment</code>.</li> </ul>"},{"location":"#accessing-the-data","title":"Accessing the Data","text":"<p>The data is stored as <code>.parquet</code> files on a S3-compatible object storage (MinIO) and can be accessed in multiple ways:</p> Method Best For Description Website Less technical users Download individual region files via a map-based interface. CLI Bulk downloads Sync entire countries or the full dataset to your local storage. Python Quick data exploration Stream specific regions into memory (GeoPandas) or filter chunks (PyArrow). SQL (DuckDB) Advanced Filtering Query the cloud data directly to download only specific subsets (e.g., buildings &gt; 50m). Zenodo Research &amp; Citations Access stable, versioned snapshots for scientific reproducibility."},{"location":"#contribute","title":"Contribute","text":"<p>You know of an open dataset that is not yet included or you've spotted a bug? Drop us an email at info(at)eubucco.com or open an issue in our GitHub repository.</p>"},{"location":"api/","title":"API","text":""},{"location":"api/#api-explorer","title":"API Explorer","text":"<p>API Explorer</p> <p>The API Explorer offers an easy way to interact and understand out API. </p>"},{"location":"api/#python-library","title":"Python Library","text":"<p>Soon we will offer our own library to download the data!</p>"},{"location":"license/","title":"License","text":"<p>The EUBUCCO dataset is licensed under the Open Data Commons Open Database License (ODbL) v1.0, with the following regional exceptions:</p> Region <code>geometry_source</code> License Constraint Prague <code>gov-czechia-prague</code> CC-BY-SA Share-Alike Abruzzo <code>gov-italy-abruzzo</code> CC-BY-NC Non-Commercial <p>If you require a dataset licensed exclusively under ODbL, filter out these sources using the <code>geometry_source</code> column.</p>"},{"location":"data-access/","title":"Accessing the Data","text":"<p>This guide outlines various methods to access and download the EUBUCCO dataset, which is hosted on S3-compatible storage (MinIO).</p>"},{"location":"data-access/#overview-of-data-access-methods","title":"Overview of Data Access Methods","text":"Method Best For Description Website Less technical users Download individual region files via a map-based interface. CLI Bulk downloads Sync entire countries or the full dataset to your local storage. Python Quick data exploration Stream specific regions into memory (GeoPandas) or filter chunks (PyArrow). SQL (DuckDB) Advanced Filtering Query the cloud data directly to download only specific subsets (e.g., buildings &gt; 50m). Zenodo Research &amp; Citations Access stable, versioned snapshots for scientific reproducibility."},{"location":"data-access/cli/","title":"\ud83d\udcbb Command Line Interface (CLI)","text":"<p>EUBUCCO data is hosted on an S3-compatible object store and can be accessed using standard command-line tools. Below are several options depending on your setup and preferences.</p>"},{"location":"data-access/cli/#1-using-aws-cli","title":"1. Using AWS CLI","text":"<p>The standard AWS CLI can be used to access EUBUCCO data by specifying the custom endpoint and using the <code>--no-sign-request</code> flag for anonymous access.</p> Download single regionDownload entire dataset <pre><code># Example: Download Liguria (ITC3)\naws s3 cp s3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet . \\\n    --endpoint-url https://dev-s3.eubucco.com \\\n    --no-sign-request\n</code></pre> <pre><code># Example: Download the entire v0.2 dataset\naws s3 cp s3://eubucco/v0.2/buildings/parquet/ ./eubucco-data/ \\\n    --endpoint-url https://dev-s3.eubucco.com \\\n    --no-sign-request \\\n    --recursive\n</code></pre>"},{"location":"data-access/cli/#2-using-minio-client","title":"2. Using MinIO Client","text":"<p>The MinIO CLI <code>mc</code> is another convenient option for interacting with S3-compatible storage services.</p> <p>Setup: <pre><code>mc alias set eubucco https://dev-s3.eubucco.com \"\" \"\"\n</code></pre></p> Download single regionDownload entire dataset <pre><code>mc cp eubucco/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet .\n</code></pre> <pre><code>mc cp --recursive eubucco/eubucco/v0.2/buildings/parquet/ .\n</code></pre>"},{"location":"data-access/cli/#3-using-standard-network-utilities","title":"3. Using Standard Network Utilities","text":"<p>For users on systems without S3 clients, standard network utilities provide a lightweight alternative for direct file retrieval.</p> wgetcURL <pre><code>wget -c --show-progress https://dev-s3.eubucco.com/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\n</code></pre> <pre><code>curl -OC - --retry 5 https://dev-s3.eubucco.com/eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\n</code></pre>"},{"location":"data-access/python/","title":"\ud83d\udc0d Python (GeoPandas / PyArrow)","text":"<p>Python is ideal for quickly streaming data directly into memory for exploration or small-scale analysis.</p>"},{"location":"data-access/python/#1-using-geopandas","title":"1. Using GeoPandas","text":"<p>GeoPandas uses <code>fsspec</code> under the hood to handle S3 paths. Use <code>storage_options</code> to point to the EUBUCCO endpoint.</p> Download single region <pre><code>import geopandas as gpd\n\nstorage_opts = {\n    \"anon\": True,\n    \"client_kwargs\": {\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n}\n\n# Example: Download Liguria (ITC3)\npath = \"s3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet\"\ngdf = gpd.read_parquet(path, storage_options=storage_opts)\n</code></pre>"},{"location":"data-access/python/#2-using-pyarrow","title":"2. Using PyArrow","text":"<p>PyArrow is significantly more efficient for large data chunks because it only loads the specific partitions or rows you filter for (predicate pushdown).</p> Download single regionDownload larger chunks <pre><code>import fsspec\nimport pyarrow.dataset as ds\n\nfs = fsspec.filesystem(\n    protocol=\"s3\",\n    anon=True,\n    client_kwargs={\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n)\n\n# Example: Download Liguria (ITC3)\ndataset = ds.dataset(\n    source=\"eubucco/v0.2/buildings/parquet/nuts_id=ITC3\",\n    filesystem=fs,\n    format=\"parquet\",\n)\ntable = dataset.to_table()\n</code></pre> <pre><code>import fsspec\nimport pyarrow.dataset as ds\n\nfs = fsspec.filesystem(\n    protocol=\"s3\",\n    anon=True,\n    client_kwargs={\"endpoint_url\": \"https://dev-s3.eubucco.com\"}\n)\n\ndataset = ds.dataset(\n    source=\"eubucco/v0.2/buildings/parquet/\",\n    filesystem=fs,\n    format=\"parquet\"\n)\n\n# Example: Get all data for Italy (regions starting with \"IT\")\ntable = dataset.to_table(filter=ds.field(\"nuts_id\").starts_with(\"IT\"))\n</code></pre> <pre><code># Optionally convert to DataFrame or GeoDataFrame\ndf = table.to_pandas()\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/sql/","title":"\ud83e\udd86 DuckDB (SQL)","text":"<p>DuckDB offers the most powerful way to query EUBUCCO. It treats the S3 bucket like a database, letting you filter by geography before any data is actually downloaded.</p>"},{"location":"data-access/sql/#1-using-sql","title":"1. Using SQL","text":""},{"location":"data-access/sql/#configuration","title":"Configuration","text":"<p>Install spatial extension: <pre><code>INSTALL httpfs; LOAD httpfs;\nINSTALL spatial; LOAD spatial\n</code></pre></p> <p>Configure EUBUCCO's MinIO endpoint: <pre><code>SET s3_endpoint='dev-s3.eubucco.com';\nSET s3_url_style='path';\n</code></pre></p>"},{"location":"data-access/sql/#data-access","title":"Data access","text":"Download single regionDownload larger chunks <pre><code>SELECT * FROM 's3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet' \nLIMIT 10\n</code></pre> <p>By using hive_partitioning, DuckDB understands the folder structure (e.g., <code>nuts_id=...</code>) and can filter hundreds of files instantly.</p> <pre><code>-- Example: Query all of France (FR) for buildings higher than 50 meters\nSELECT id, height, ST_AsText(geometry) \nFROM read_parquet(\n    's3://eubucco/v0.2/buildings/parquet/**/*.parquet',\n    hive_partitioning = true\n)\nWHERE nuts_id LIKE 'FR%' \nAND height &gt; 50;\n</code></pre>"},{"location":"data-access/sql/#2-using-duckdbs-python-library","title":"2. Using DuckDB's Python library","text":"<pre><code>pip install duckdb\n</code></pre> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.execute(\"INSTALL httpfs; LOAD httpfs;\")\ncon.execute(\"INSTALL spatial; LOAD spatial;\")\n\ncon.execute(\"SET s3_endpoint='dev-s3.eubucco.com';\") \ncon.execute(\"SET s3_url_style='path';\")\n</code></pre> <pre><code>query = \"\"\"\nSELECT * EXCLUDE geometry, ST_AsWKB(geometry) AS geometry FROM 's3://eubucco/v0.2/buildings/parquet/nuts_id=ITC3/ITC3.parquet' \nLIMIT 10\n\"\"\"\n# Fetch data with WKB-encoded geometries\ndf = con.execute(query).arrow().to_pandas()\n</code></pre> <pre><code># Optionally convert to GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/website/","title":"\ud83d\uddb1\ufe0f File Downloads via Website","text":"<p>The EUBUCCO Data Portal is the most user-friendly way to browse and download data.</p> <ul> <li>Formats: Available as <code>.parquet</code>, <code>.gpkg</code>, and <code>.shp</code> files</li> <li>Partitioning: The dataset is split into multiple files, each containing all buildings within a single NUTS2 region.</li> <li>Batch download: Larger regions (e.g. federal states or entire countries) can be downloaded as ZIP archives containing all regional NUTS2 files.</li> <li>Metadata: Additional files include information on source datasets, type matching, regional statistics, and more.</li> </ul>"},{"location":"data-access/website/#reading-downloaded-parquet-files","title":"Reading downloaded Parquet files","text":"Read single file (GeoPandas)Read multiple files (GeoPandas)Read multiple files (DuckDB) <pre><code>import geopandas as gpd\n\npath = \"/path/to/downloaded/file.parquet\"\ngdf = gpd.read_parquet(path)\n</code></pre> <pre><code>import geopandas as gpd\n\npath = \"/path/to/downloaded/unzipped/directory\"\ngdf = gpd.read_parquet(path)\n</code></pre> <pre><code>import duckdb\n\ncon = duckdb.connect()\ncon.execute(\"INSTALL spatial; LOAD spatial;\")\n</code></pre> <pre><code># Read data with WKB-encoded geometries\nquery = \"\"\"\nSELECT\n    * EXCLUDE geometry,\n    ST_AsWKB(geometry) AS geometry\nFROM read_parquet('/path/to/your/folder/*.parquet')\n\"\"\"\ndf = con.execute(query).arrow().to_pandas()\n</code></pre> <pre><code># Optionally convert to GeoDataFrame\ngdf = gpd.GeoDataFrame(\n    df,\n    geometry=gpd.GeoSeries.from_wkb(df[\"geometry\"]),\n    crs=\"EPSG:3035\",\n)\n</code></pre>"},{"location":"data-access/zenodo/","title":"\ud83d\udcc2 Scientific Archiving (Zenodo)","text":"<p>For research projects requiring a permanent, citable version of the data, EUBUCCO is archived on Zenodo.</p> <ul> <li>DOI: 10.5281/zenodo.7225259</li> <li>Usage: Ideal for ensuring your research results can be reproduced by others using the exact same data snapshot.</li> <li>Bulk Archive: A full database ZIP (~100 GB) containing all geometries and attributes is available here.</li> </ul>"},{"location":"data-format/","title":"Data Format","text":""},{"location":"data-format/#data-schema","title":"Data Schema","text":"<p>The core dataset structure is organized into five functional groups:</p> <ul> <li>Identifiers: Unique building <code>id</code> and geographic identifies: <code>region_id</code> (NUTS3) and <code>city_id</code> (LAU).</li> <li>Attributes: Building characteristics (<code>type</code>, <code>subtype</code>, <code>height</code>, <code>floors</code>, <code>year</code>).</li> <li>Confidence: Uncertainty of ML-based estimation and cross-source merging of attributes.</li> <li>Sources: Provenance tracking for every attribute (i.e. <code>osm</code>, <code>msft</code>, <code>gov-*</code>, <code>estimated</code>).</li> <li>Geometry: <code>ETRS89</code> footprints stored in Well-Known Binary (WKB) format.</li> </ul>"},{"location":"data-format/#file-format","title":"File Format","text":"<p>The EUBUCCO dataset is distributed in multiple file formats:</p> <ul> <li>Parquet (<code>.parquet</code>)</li> <li>GeoPackage (<code>.gpkg</code>)</li> <li>Shapefile (<code>.shp</code>)</li> </ul>"},{"location":"data-format/#metadata-files","title":"Metadata Files","text":"<p>In addition to the core building data, metadata information with respect to the source datasets, data retrieval dates, and attribute harmonization (e.g. building type mapping and aggregation) are provided as additional files.</p>"},{"location":"data-format/#uncertainty","title":"Uncertainty","text":"<p>For attributes that were estimated using machine learning techniques or merged from OpenStreetMap data, we provide an uncertainty quantification.</p>"},{"location":"data-format/format/","title":"Data File Format","text":"<p>The EUBUCCO dataset is natively optimized for Parquet. While we provide other formats for broad compatibility and convenience, the Parquet distribution is the primary format designed to handle continental-scale analysis with high performance.</p> Parquet GeoPackage Shapefile Support Level Recommended Experimental Experimental Performance Excellent Good Fair Compression High Medium Low Querying Columnar (Best) SQL/Indexed (Good) Flat-file (Poor) Selective Reads Yes (Columns &amp; Rows) Yes (via SQL) No (Full file load)"},{"location":"data-format/format/#parquet-parquet","title":"Parquet (<code>.parquet</code>)","text":"<p>An open-source, columnar storage format optimized for efficient data storage and retrieval.</p> <ul> <li>Best for: Large-scale analysis using Python (Pandas/GeoPandas), R, or Big Data tools.</li> <li>Pros: Smallest file size, fastest to read, and supports selective reads and predicate pushdown filtering.</li> <li>Cons: Limited native support in older GIS desktop software without plugins.</li> </ul> <p>Recommended</p> <p>Native format of the dataset.</p>"},{"location":"data-format/format/#geopackage-gpkg","title":"GeoPackage (<code>.gpkg</code>)","text":"<p>An open, non-proprietary, SQLite-based container for geospatial data.</p> <ul> <li>Best for: General GIS work in QGIS, ArcGIS Pro, or mobile mapping apps.</li> <li>Pros: Single-file convenience, widely compatible, and supports large datasets better than Shapefiles.</li> <li>Cons: Increased file size and slower read speeds compared to Parquet.</li> </ul> <p>Experimental</p> <p>Provided as a convenience for desktop GIS users. Minor typing discrepancies may occur during the automated conversion from the native Parquet source.</p>"},{"location":"data-format/format/#shapefile-shp","title":"Shapefile (<code>.shp</code>)","text":"<p>The legacy industry standard for geospatial vector data.</p> <ul> <li>Best for: Compatibility with older legacy software.</li> <li>Pros: Universal support across almost every GIS tool ever made.</li> <li>Cons: Large files, slow reading, and limited to 2GB per file. No querying possible; all data must be read into memory.</li> </ul> <p>Experimental</p> <p>Provided for legacy GIS software compatibility but generally not recommended. Automated conversion from Parquet may cause minor typing discrepancies. Crucially, some EUBUCCO regions exceed the 2GB limit. Column names differ from Parquet and GeoPackage files due to 10 character limit. Some values have been truncated to 264 characters.</p>"},{"location":"data-format/metadata/","title":"Metadata Files","text":"<p>Outdated</p> <p>Metadata files below correspond to EUBUCCO v0.1.</p> <ol> <li>Metadata table on input datasets (<code>input\u2212dataset\u2212metatable\u2212v0.1.xlsx</code>): This table contains 38 dimensions that provide users with the main information about input datasets. Specifically, the file contains the input dataset\u2019s<ul> <li>name and area information (e.g. country, dataset specific area and dataset name)</li> <li>meta info (e.g. access date, data owner, license, link to ressource or download approach)</li> <li>structure (e.g. file format, breakdown or additional files matched for attributes)</li> <li>content relevant to EUBUCCO v0.1 (e.g. availability of given attributes or LoDs)</li> <li>variable names (e.g. ID, construction year, or building element for <code>.gml</code> files)</li> <li>and validation information via the number of buildings at three stages of the workflow (after parsing, cleaning and matching with administrative boundaries) together with the losses that occurred and a short explanation in case of large losses</li> </ul> </li> <li>Table on excluded datasets (<code>excluded\u2212datasets\u2212v0.1.xlsx</code>): This table provides an overview of available government datasets that were not included in this study with a rational why, most often because they were only available at a high cost. For all these datasets we contacted the data owner to ask whether the data were available for free for research; the status of the dataset reflects their answer and a contact date is also indicated in the file</li> <li>Database content metrics at the city-level (<code>city\u2212level\u2212overview\u2212tables\u2212v0.1.zip</code>): The overview files provide 48 city-level metrics for all 41,456 cities, of which 627, mostly very small cities, do not contain any building. The files enable a detailed overview of the database content in term of geometries and attributes can be used to study patterns across regions and countries and can also be used to identify bugs or outliers. They are provided as a table for each country with the following naming: <code>&lt;country&gt;_overview\u2212v0.1.csv</code>. Each table contains:<ul> <li>the city ID, name and region</li> <li>building counts and footprint metrics including total number of buildings, total footprint area, footprint area distribution, max footprint area, number of 0-m<sup>2</sup> footprints, etc.</li> <li>height distribution metrics in relative and absolute terms, including overall metrics e.g. median and max value, also outliers outside of a reasonable range e.g. negative values, and metrics by height bins e.g. <code>[3, 5(</code> or <code>[11, 15(</code></li> <li>type distribution metrics in relative and absolute terms computed for the variable type and describing the proportion of residential, non-residential and unknown building types</li> <li>construction year distribution metrics in relative and absolute terms grouped by construction year bins e.g. <code>[1801, 1900(</code> or <code>[1951, 2000(</code> and also counting additional dimensions such as outliers outside of a reasonable range e.g. negative values</li> </ul> </li> <li>Type matching tables (<code>type_matches\u2212v0.1.zip</code>): Multiple tables are provided for each relevant dataset or group of datasets (if cadaster codes apply for several datasets in Germany and Italy) as <code>&lt;dataset&gt;\u2212type_matches\u2212v0.1.csv</code>. These tables enable to map the type of the raw data (<code>type_source</code>) to the type column (<code>type_source</code>) of the database and provide an English translation of the type of the raw data</li> <li>Administrative code matching table (<code>admin\u2212codes\u2212matches\u2212v0.1.csv</code>): This table enables to match the GADM code from building ids with its country, region, city and the input dataset per city.</li> <li>Administrative city levels (<code>gadm\u2212city\u2212levels\u2212v0.1.csv</code>): This table provides an overview of the GADM level that was chosen to define the city level per country.</li> </ol>"},{"location":"data-format/schema/","title":"Data Schema Reference","text":""},{"location":"data-format/schema/#core-data-fields","title":"Core Data Fields","text":"Group Attribute Type Definition Example Identifiers <code>id</code> <code>string</code> Unique ID composed of a Block ID<sup>1</sup> and sequence number. <code>3ba70f9963714924-0</code> <code>region_id</code> <code>string</code> NUTS3 regional identifier. <code>FR931</code> <code>city_id</code> <code>string</code> Local Administrative Unit (LAU) identifier for the city. <code>FR93066</code> Attributes <code>type</code> <code>category</code> Binary usage type<sup>2</sup>. <code>residential</code> <code>subtype</code> <code>category</code> Detailed usage type<sup>3</sup>. <code>terraced</code> <code>height</code> <code>float</code> Distance in meters from ground floor to top of building. <code>23.9</code> <code>floors</code> <code>float</code> Total number of above-ground floors. <code>3.5</code> <code>construction_year</code> <code>integer</code> Year initial construction finished (not renovation year). <code>1963</code> Geometry <code>geometry</code> <code>binary</code> Footprint geometry projected in <code>ETRS89</code> (<code>EPSG:3035</code>) encoded as WKB. Units in meters. <code>01030000...</code>"},{"location":"data-format/schema/#auxiliary-data-fields","title":"Auxiliary Data Fields","text":"Group Attribute Type Definition Example Confidence <code>type_confidence</code> <code>float</code> Area-weighted intersection ratio (merged) or sum of calibrated subtype probabilities (predicted). Range: [0, 1]. <code>0.95</code> <code>subtype_confidence</code> <code>float</code> Area-weighted intersection ratio (merged) or calibrated class probability (predicted). Range: [0, 1]. <code>0.64</code> <code>height_confidence_lower</code> <code>float</code> Min source value (merged) or lower 95% bootstrap CI (predicted) for height. <code>7.5</code> <code>height_confidence_upper</code> <code>float</code> Max source value (merged) or upper 95% bootstrap CI (predicted) for height. <code>10.5</code> <code>floors_confidence_lower</code> <code>float</code> Min source value (merged) or lower 95% bootstrap CI (predicted) for floors. <code>2.7</code> <code>floors_confidence_upper</code> <code>float</code> Max source value (merged) or upper 95% bootstrap CI (predicted) for floors. <code>3.2</code> <code>construction_year_confidence_lower</code> <code>int</code> Min source year (merged) or lower 95% bootstrap CI (predicted) for year. <code>1990</code> <code>construction_year_confidence_upper</code> <code>int</code> Max source year (merged) or upper 95% bootstrap CI (predicted) for year. <code>2000</code> Sources <code>geometry_source</code> <code>category</code> Origin of footprint geometry. <code>gov-france</code> <code>type_source</code> <code>category</code> Origin of <code>type</code> attribute. <code>osm</code> <code>subtype_source</code> <code>category</code> Origin of <code>subtype</code> attribute. <code>estimated</code> <code>height_source</code> <code>category</code> Origin of <code>height</code> attribute. <code>estimated</code> <code>floors_source</code> <code>category</code> Origin of <code>floors</code> attribute. <code>gov-france</code> <code>construction_year_source</code> <code>category</code> Origin of <code>construction_year</code> attribute. <code>gov-france</code> Source IDs <code>geometry_source_id</code> <code>string</code> Primary identifier from the source provider. <code>BATIMENT000...</code> <code>type_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>type</code> attribute. <code>['osm_123']</code> <code>subtype_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>subtype</code> attribute. <code>['osm_123']</code> <code>height_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>height</code> attribute. <code>['ign_456']</code> <code>floors_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>floors</code> attribute. <code>['ign_456']</code> <code>construction_year_source_ids</code> <code>array</code> List of IDs from source(s) contributing to the <code>construction_year</code>. <code>['ign_456']</code> Source values <code>subtype_raw</code> <code>string</code> The original, unmapped building use type from the source dataset. <code>Einfamilienhaus</code>"},{"location":"data-format/schema/#example","title":"Example","text":"id region_id city_id type subtype height floors construction_year type_confidence subtype_confidence height_confidence_lower height_confidence_upper floors_confidence_lower floors_confidence_upper construction_year_confidence_lower construction_year_confidence_upper geometry_source type_source subtype_source height_source floors_source construction_year_source geometry_source_id type_source_ids subtype_source_ids height_source_ids floors_source_ids construction_year_source_ids subtype_raw geometry aec95c88db604a13-0 FRF31 FR54029 residential detached 3.3 1.0 nan 1.0 0.99 3.2 3.5 1.0 1.0 &lt;NA&gt; &lt;NA&gt; gov-france osm estimated estimated osm &lt;NA&gt; BATIMENT0000000334652469 ['lorraine-latest_1372503'] &lt;NA&gt; &lt;NA&gt; ['lorraine-latest_1372503'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((402...)) 9d1c73d618ef419e-0 FRF31 FR54037 residential detached 5.7 2.0 nan 0.86 0.84 5.3 6.2 2.0 2.0 &lt;NA&gt; &lt;NA&gt; gov-france osm estimated estimated osm &lt;NA&gt; BATIMENT0000002101827291 ['lorraine-latest_1661552'] &lt;NA&gt; &lt;NA&gt; ['lorraine-latest_1661552'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((404...)) ce19c9e4976d46f6-2 FRF31 FR54039 non-residential others 3.8 2.0 nan 0.84 0.84 nan nan 2.0 2.0 &lt;NA&gt; &lt;NA&gt; gov-france osm osm gov-france osm &lt;NA&gt; BATIMENT0000000334989890 ['lorraine-latest_805209'] ['lorraine-latest_805209'] &lt;NA&gt; ['lorraine-latest_804619'] &lt;NA&gt; Indiff\u00e9renci\u00e9 POLYGON ((407...)) ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ..."},{"location":"data-format/schema/#technical-notes","title":"Technical Notes","text":""},{"location":"data-format/schema/#building-type-classification","title":"Building Type Classification","text":"<ul> <li>We use a two-tier hierarchy to categorize building usage. The <code>type</code> field provides a high-level binary split, while the <code>subtype</code> field contains the granular class.<ul> <li>Type: <code>residential</code>. Subtypes: <code>detached</code> (single-family), <code>semi-detached</code> (duplex), <code>terraced</code> (row house), and <code>apartment</code> (multi-family).</li> <li>Type: <code>non-residential</code>. Subtypes: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, and <code>others</code> (e.g. garages)</li> </ul> </li> <li>The <code>subtype_raw</code> column preserves the original source classification string prior to this harmonization. Refer to the metadata file on building type mapping for more details.</li> </ul>"},{"location":"data-format/schema/#geometry-encoding","title":"Geometry Encoding","text":"<ul> <li>The <code>geometry</code> column is stored as Well-Known Binary (WKB). This is a compact, machine-readable format optimized for GIS tools like PostGIS, QGIS, and GeoPandas.</li> </ul>"},{"location":"data-format/schema/#attribute-sources","title":"Attribute Sources","text":"<ul> <li>Attribute source categories are <code>osm</code>, <code>msft</code>, <code>gov-&lt;region&gt;</code>, <code>estimated</code> (where  represents the specific regional authoritative dataset identifier)"},{"location":"data-format/schema/#attribute-uncertainty","title":"Attribute Uncertainty","text":"<ul> <li>Ground Truth: <code>NaN</code> in a <code>&lt;attr&gt;_confidence</code> column indicates the attribute was provided directly by the geometry source; no merging or machine learning estimation was required.</li> <li>Interpretation: For <code>type_confidence</code> and <code>subtype_confidence</code>, a value of 0.6 implies that 60% of buildings in that cohort are statistically expected to be correctly classified.</li> <li>Methodology: For details on how the uncertainty of attribute estimation and merging is quantified, please refer to the Uncertainty Section.</li> </ul>"},{"location":"data-format/schema/#attribute-merging","title":"Attribute Merging","text":"<ul> <li>Source Mismatch: If <code>geometry_source</code> and <code>&lt;attr&gt;_source</code> differ, the attribute has been merged between datasets.</li> <li>Data Fusion: If <code>&lt;attr&gt;_source_ids</code> contains multiple values, the final value has been from aggregated from multiple source buildings.</li> </ul> <ol> <li> <p>Block ID: A unique identifier for a cluster of topologically connected (touching) buildings.\u00a0\u21a9</p> </li> <li> <p>Binary use type categories: <code>residential</code> and <code>non-residential</code>.\u00a0\u21a9</p> </li> <li> <p>Detailed use type categories: <code>industrial</code>, <code>commercial</code>, <code>public</code>, <code>agricultural</code>, <code>others</code>, <code>detached</code>, <code>semi-detached</code>, <code>terraced</code>, and <code>apartment</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"data-format/uncertainty/","title":"Attribute Uncertainty Quantification","text":"<p>Building attributes derived from source data (e.g., OpenStreetMap) or machine learning models inherently carry degrees of uncertainty.</p> <p>To provide a transparent measure of data reliability, each attribute is accompanied by a confidence metric. This documentation details the dual-methodology approach\u2014spatial intersection ratios for merged records and classification probability / regression bootstrapping for predicted values\u2014used to quantify this uncertainty.</p>"},{"location":"data-format/uncertainty/#type-subtype","title":"Type &amp; Subtype","text":"<p>The confidence of the categorical attributes type and subtype is quantified as follows:</p>"},{"location":"data-format/uncertainty/#for-attributes-merged-from-osm-footprint-intersection-ratio","title":"For Attributes Merged From OSM: Footprint Intersection Ratio","text":"<p>When an attribute is merged from a source dataset (i.e. OSM) onto a target building footprint, the confidence is calculated as the Intersection over Area (IoA):</p> \\[\\text{Confidence} = \\frac{\\text{Area}(B_{target} \\cap B_{source})}{\\text{Area}(B_{target})}\\] <ul> <li>1.0: Perfect spatial overlap.</li> <li>&lt; 1.0: Partial overlap, suggesting potential mismatch or misaligned geometries.</li> </ul>"},{"location":"data-format/uncertainty/#for-attributes-predicted-using-ml-calibrated-classification-probabilities","title":"For Attributes Predicted using ML: Calibrated Classification Probabilities","text":"<p>For attributes generated by our classification models, the confidence is the calibrated probability of the predicted class:</p> <ul> <li>Subtype Confidence: The output of the model after applying calibration to ensure the probability reflects real-world accuracy.</li> <li> <p>Type Confidence: Since type (<code>residential</code>/<code>non-residential</code>) is an aggregate, its confidence is the sum of the calibrated probabilities of all subtypes belonging to that category:</p> \\[P(\\text{Type}) = \\sum P(\\text{Subtypes} \\in \\text{Type})\\] </li> </ul>"},{"location":"data-format/uncertainty/#height-floors-construction-year","title":"Height, Floors, Construction Year","text":"<p>The confidence of the numerical attributes height, floors and construction year is quantified as follows:</p>"},{"location":"data-format/uncertainty/#for-attributes-merged-from-osm-value-extremes","title":"For Attributes Merged From OSM: Value Extremes","text":"<p>If multiple source buildings match a single target footprint with different values:</p> <ul> <li>Lower: The minimum value found among all matching sources.</li> <li>Upper: The maximum value found among all matching sources.</li> </ul>"},{"location":"data-format/uncertainty/#for-attributes-predicted-using-ml-bootstrapped-95-ci","title":"For Attributes Predicted using ML: Bootstrapped 95% CI","text":"<p>To quantify the uncertainty of our regression models, we use a bootstrap approach:</p> <ol> <li>Bootstrap Sampling: The model is run \\(n=10\\) times using different seeds or data subsamples, resulting in a set of predictions \\(Y = \\{y_1, y_2, \\dots, y_{10}\\}\\).</li> <li> <p>Standard Error Calculation: We calculate the sample mean \\(\\bar{y}\\) and the standard error of the mean (SEM):</p> \\[\\text{SEM} = \\frac{s}{\\sqrt{n}}\\] <p>where \\(s\\) is the sample standard deviation.</p> </li> <li> <p>Interval Calculation: The confidence bounds are defined using the \\(t\\)-statistic for \\(n-1\\) degrees of freedom (at \\(\\alpha = 0.05\\)):</p> <p>Lower Confidence:</p> \\[\\text{lower} = \\bar{y} - t_{0.975, 9} \\times \\text{SEM}\\] <p>Upper Confidence:</p> \\[\\text{upper} = \\bar{y} + t_{0.975, 9} \\times \\text{SEM}\\] </li> </ol>"},{"location":"data-usage/cookbook/","title":"Cookbook: Python &amp; SQL","text":"<p>This reference provides essential snippets for cleaning, enriching, and transforming EUBUCCO data using GeoPandas or DuckDB.</p>"},{"location":"data-usage/cookbook/#identifiers","title":"Identifiers","text":""},{"location":"data-usage/cookbook/#determining-block-id","title":"Determining Block ID","text":"<p>EUBUCCO IDs are formatted as <code>{uuid}-{index}</code>. The prefix can be extracted to identify building blocks, i.e. clusters of adjacent buildings.</p> GeoPandasDuckDB <pre><code>gdf['block_id'] = gdf['id'].str.split('-').str[0]\n</code></pre> <pre><code>SELECT SPLIT_PART(id, '-', 1) AS block_id FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#determining-nuts-0-1-or-2-region","title":"Determining NUTS 0, 1, or 2 Region","text":"GeoPandasDuckDB <pre><code>gdf['NUTS_2'] = gdf['region_id'].str[:4]\ngdf['NUTS_1'] = gdf['region_id'].str[:3]\ngdf['country'] = gdf['region_id'].str[:2]  # EU VAT 2-digit country code\n</code></pre> <pre><code>SELECT LEFT(region_id, 2) AS country FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#attribute-metadata","title":"Attribute Metadata","text":""},{"location":"data-usage/cookbook/#handling-confidence-values","title":"Handling Confidence Values","text":"<p>Authoritative data lacks explicit confidence scores. Fill these gaps with 1.0 to ensure they are not excluded during quality filters.</p> GeoPandasDuckDB <pre><code>gdf[\"type_confidence\"] = gdf[\"type_confidence\"].fillna(1.0)\n</code></pre> <pre><code>SELECT COALESCE(type_confidence, 1.0) as type_confidence FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#attribute-source-comparison","title":"Attribute Source Comparison","text":"<p>Determine if an attribute was merged from an external source or estimated using ML by identifying mismatches between geometry and attribute sources.</p> GeoPandasDuckDB <pre><code>gdf[\"is_inferred\"] = gdf[\"geometry_source\"] != gdf[\"type_source\"]\n</code></pre> <pre><code>SELECT *, (geometry_source != type_source) AS is_inferred FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#custom-building-type-harmonization","title":"Custom Building Type Harmonization","text":"<p>Map raw subtypes from source datasets to custom building type classification.</p> GeoPandasDuckDB <pre><code>osm_map = {\n    'apartments': 'high_density',\n    'detached': 'low_density',\n    'retail': 'economic'\n}\ngdf['urban_function'] = gdf['subtype_raw'].map(osm_map).fillna('unclassified')\n</code></pre> <pre><code>SELECT \n    subtype_raw,\n    CASE \n        WHEN subtype_raw IN ('apartments', 'terrace') THEN 'high_density'\n        WHEN subtype_raw IN ('detached', 'house') THEN 'low_density'\n        WHEN subtype_raw IN ('retail', 'office') THEN 'economic'\n        ELSE 'unclassified' \n    END AS urban_function\nFROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#geometry","title":"Geometry","text":""},{"location":"data-usage/cookbook/#decoding-wkb-and-wkt","title":"Decoding WKB and WKT","text":"<p>Geometries are stored as Well-Known Binary (WKB). Use these methods for manual decoding or to export human-readable Well-Known Text (WKT).</p> Pandas (WKB Parsing)DuckDB (WKB Export)DuckDB (WKT Export) <pre><code>import pandas as pd\nimport geopandas as gpd\nfrom shapely import wkb\n\n# Load raw parquet (geometry is binary)\ndf = pd.read_parquet(\"data.parquet\")\n\n# Fast decoding of WKB column to Shapely objects\ngdf = gpd.GeoDataFrame(\n    df, \n    geometry=gpd.GeoSeries.from_wkt(df[\"geometry\"]),\n    # OR geometry=df[\"geometry\"].apply(wkb.loads),\n    crs=\"EPSG:3035\"\n)\n</code></pre> <pre><code>SELECT id, ST_AsWKB(geometry) AS geometry FROM buildings;\n</code></pre> <pre><code>-- Convert binary to human-readable strings\nSELECT id, ST_AsText(geometry) AS geometry FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#coordinate-reference-system-crs-transformation","title":"Coordinate Reference System (CRS) Transformation","text":"<p>Convert building geometries from the local projected CRS (<code>EPSG:3035</code>) to WGS84 (Lat/Lng).</p> GeoPandasDuckDB <pre><code>gdf = gdf.to_crs(epsg=4326)\n</code></pre> <pre><code>SELECT \n    * EXCLUDE geometry, \n    ST_Transform(geometry, 'EPSG:3035', 'EPSG:4326') AS geometry \nFROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#centroid-generation","title":"Centroid Generation","text":"<p>Replace building footprints with centroids to reduce computational overhead for point-in-polygon operations or visualization.</p> GeoPandasDuckDB <pre><code>gdf[\"geometry\"] = gdf.centroid\n</code></pre> <pre><code>SELECT * EXCLUDE geometry, ST_Centroid(geometry) AS geometry FROM buildings;\n</code></pre>"},{"location":"data-usage/cookbook/#h3-grid-aggregation","title":"H3 Grid Aggregation","text":"<p>Map buildings to hexagonal grid (H3) for analysis.</p> Python (h3pandas)Python (h3-py + GeoPandas)DuckDB <pre><code>import h3pandas\n# Automatically handles centroid extraction and CRS shift to WGS84\n# 'res=9' provides a spatial resolution of ~0.1km\u00b2\nh3_gdf = gdf.h3.geo_to_h3_aggregate(res=9, operation='count')\n</code></pre> <pre><code>import h3\n# Manual approach: Transform -&gt; Centroid -&gt; Map\ngdf['h3_9'] = gdf.centroid.to_crs(epsg=4326).apply(\n    lambda g: h3.latlng_to_cell(g.y, g.x, 9)\n)\nh3_stats = gdf.groupby('h3_9').size()\n</code></pre> <pre><code>-- Transform to EPSG:4326 within the H3 function call\nSELECT \n    h3_latlng_to_cell(\n        ST_Y(ST_Transform(ST_Centroid(geometry), 'EPSG:3035', 'EPSG:4326')), \n        ST_X(ST_Transform(ST_Centroid(geometry), 'EPSG:3035', 'EPSG:4326')), \n        9\n    ) AS cell,\n    COUNT(*) AS count\nFROM buildings\nGROUP BY cell;\n</code></pre>"},{"location":"data-usage/loading/","title":"Efficient Data Loading &amp; Filtering","text":"<p>Filtering at the data-loading stage is the most efficient way to extract specific building subsets based on quality, source, or geography. By using predicate pushdown across partitioned files, you avoid reading unnecessary data into memory.</p>"},{"location":"data-usage/loading/#spatial-filtering","title":"Spatial Filtering","text":""},{"location":"data-usage/loading/#bounding-box-bbox-filtering","title":"Bounding Box (bbox) Filtering","text":"<p>Limit the dataset to a specific geographic extent using helper columns. </p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\nimport glob\n\n# GeoPandas only reads the subsets of the data that intersect the bbox\nbbox = (5300000, 1880000, 5400000, 1920000)\ngdf = gpd.read_parquet(\"eubucco_data/\", bbox=bbox)\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\n\ntable = dataset.to_table(filter=(\n    (ds.field(\"bbox.xmin\") &gt;= 5300000) &amp; (ds.field(\"bbox.xmax\") &lt;= 5400000) &amp;\n    (ds.field(\"bbox.ymin\") &gt;= 1880000) &amp; (ds.field(\"bbox.ymax\") &lt;= 1920000)\n))\n</code></pre> <pre><code>-- Use the glob pattern '**/*.parquet' to scan all regional files\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE bbox.xmin &gt;= 5300000 AND bbox.xmax &lt;= 5400000\n  AND bbox.ymin &gt;= 1880000 AND bbox.ymax &lt;= 1920000;\n</code></pre>"},{"location":"data-usage/loading/#administrative-regions-filtering","title":"Administrative Regions Filtering","text":"<p>Limit the dataset to specific cities, regions, or countries using the partition keys and <code>region_id</code> (NUTS 0/1/2) or <code>city_id</code> row group metadata.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\n# Filter for a specific city\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"city_id\", \"==\", \"EL22040104\")])\n\n# Filter for all regions e.g. in Greece [EL]\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"region_id\", \"&gt;=\", \"EL\"), (\"region_id\", \"&lt;\", \"EM\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\n\n# Filter for a specific city\ncity_table = dataset.to_table(filter=ds.field(\"city_id\") == \"EL22040104\")\n\n# Filter for all regions (e.g. in Greece [EL])\ncountry_table = dataset.to_table(filter=ds.field(\"region_id\").starts_with(\"EL\"))\n</code></pre> <pre><code>-- Querying a specific city\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE city_id = 'EL22040104';\n\n-- Querying all regions e.g. in Greek [EL]\nSELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE region_id LIKE 'EL%';\n</code></pre>"},{"location":"data-usage/loading/#source-filtering","title":"Source Filtering","text":""},{"location":"data-usage/loading/#filtering-by-footprint-source","title":"Filtering by Footprint Source","text":"<p>Discard ML-derived footprints (i.e. Microsoft Footprints).</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"geometry_source\", \"!=\", \"msft\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=ds.field(\"geometry_source\") != \"msft\")\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source != 'msft';\n</code></pre> <p>Filter for governmental data / discard non-authoritative sources (i.e. OSM and Microsoft ML).</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"geometry_source\", \"not in\", [\"msft\", \"osm\"])])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=~ds.field(\"geometry_source\").isin([\"msft\", \"osm\"]))\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source NOT IN ('msft', 'osm');\n</code></pre>"},{"location":"data-usage/loading/#filtering-by-attribute-source","title":"Filtering by Attribute Source","text":"<p>Isolate records where the specific attribute was not estimated using ML.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\", filters=[(\"type_source\", \"!=\", \"estimated\")])\n</code></pre> <pre><code>import pyarrow.dataset as ds\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=ds.field(\"type_source\") != \"estimated\")\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE type_source != 'estimated';\n</code></pre> <p>Isolate records where the specific attribute comes from the same source as the footprint geometry (i.e. the attribute was neither merged nor estimated using ML).</p> DuckDB <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \nWHERE geometry_source = type_source;\n</code></pre>"},{"location":"data-usage/loading/#confidence-filtering","title":"Confidence Filtering","text":"<p>Discard buildings with merged or estimated attributes that carry high uncertainty. We treat authoritative data (where confidence is <code>NaN</code>) as 100% certain.</p> <p>Performance: Since data isn't partitioned or sorted by confidence, metadata-based skipping is unavailable. However, filtering while reading in small buffers keeps memory usage low.</p> GeoPandasPyArrowDuckDB <pre><code>import geopandas as gpd\n\ngdf = gpd.read_parquet(\"eubucco_data/\")\n\n# Categorical: Type confidence &gt; 80%\nhigh_conf = gdf[gdf[\"type_confidence\"].fillna(1.0) &gt; 0.8]\n\n# Numerical: Precise height (uncertainty interval &lt; 2m)\nprecise_height = gdf[(gdf[\"height_confidence_upper\"] - gdf[\"height_confidence_lower\"]) &lt; 2.0]    \n</code></pre> <pre><code>import pyarrow.dataset as ds\n\n# Categorical: Type confidence &gt; 80%\nprecise_type = (ds.field(\"type_confidence\") &gt; 0.8) | ds.field(\"type_confidence\").is_null()\n\n# Numerical: Precise height (uncertainty interval &lt; 2m)\nheight_spread = ds.field(\"height_confidence_upper\") - ds.field(\"height_confidence_lower\")\nprecise_height = (height_spread &lt; 2.0) | ds.field(\"height_confidence_upper\").is_null()\n\n\ndataset = ds.dataset(\"eubucco_data/\")\ntable = dataset.to_table(filter=(precise_type &amp; precise_height))\n</code></pre> <pre><code>SELECT * FROM read_parquet('eubucco_data/**/*.parquet') \n-- Categorical confidence\nWHERE COALESCE(type_confidence, 1.0) &gt; 0.8 \n-- Numerical confidence\nAND (height_confidence_upper - height_confidence_lower) &lt; 2.0;\n</code></pre>"}]}